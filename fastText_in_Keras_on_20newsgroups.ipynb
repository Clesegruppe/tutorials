{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# fastText-like text classification in Keras on 20newsgroups\n",
    "\n",
    "fastText:\n",
    "* Basically word embeddings + averaging layer trained for classification\n",
    "* https://arxiv.org/abs/1607.01759\n",
    "* Very fast, but not many options to change the model (no regularization, etc.)\n",
    "\n",
    "Keras:\n",
    "* Library on top of tensorflow / theano\n",
    "* https://keras.io\n",
    "* Version used: 1.2.1\n",
    "\n",
    "20 newsgroups:\n",
    "* http://scikit-learn.org/stable/datasets/twenty_newsgroups.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 11314\n",
      "Testing samples: 7532\n",
      "\n",
      "\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Label: rec.autos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "target_names = newsgroups_train.target_names\n",
    "\n",
    "train_texts = newsgroups_train.data\n",
    "train_labels = newsgroups_train.target\n",
    "\n",
    "test_texts = newsgroups_test.data\n",
    "test_labels = newsgroups_test.target\n",
    "\n",
    "print(\"Training samples: %s\" % len(train_texts))\n",
    "print(\"Testing samples: %s\" % len(test_texts))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(train_texts[0])\n",
    "print(\"Label: %s\" % target_names[train_labels[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transforming the samples\n",
    "\n",
    "* We fit a [Tokenizer](https://keras.io/preprocessing/text/#tokenizer) and use it to convert texts to sequences of integers, which are the input to the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 134142\n",
      "['from', 'lerxst', 'wam', 'umd', 'edu', \"where's\", 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac3', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', '15', 'i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', 'it', 'was', 'a', '2', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', 'early', '70s', 'it', 'was', 'called', 'a', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'i', 'know', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'e', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n",
      "[14, 15260, 3345, 118, 441, 15, 780, 10494, 29, 1935, 1366, 7166, 1060, 361, 644, 1060, 361, 12, 1935, 1366, 1365, 412, 1935, 5973, 1366, 1887, 61, 8, 117, 5963, 75829, 34, 77, 3, 441, 32, 287, 95, 78, 91, 3345, 118, 441, 15, 4, 1741, 224, 3, 7794, 7014, 65, 6902, 79, 1935, 1366, 9881, 21, 2675, 79, 3064, 12, 17, 7166, 181, 365, 4, 4743, 369, 11806, 58, 3064, 22, 1, 4067, 601, 488, 24128, 1792, 5120, 488, 873, 16, 1003, 6, 9715, 2507, 19955, 2660, 3, 3228, 677, 263, 1355, 472, 5673, 22, 1042, 6, 24, 68, 46, 6418, 19, 712, 5507, 8, 45, 18, 19304, 7, 1, 351, 138, 533, 56, 181, 873, 2, 1, 640, 959, 1142, 27, 13, 21, 378, 1, 1366, 1887, 6, 828, 3248, 17, 7166, 206, 780, 10494, 15260, 118, 441, 15]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "\n",
    "tk = Tokenizer()\n",
    "train_texts_str = [x.encode(\"utf8\") for x in train_texts]\n",
    "tk.fit_on_texts(train_texts_str)\n",
    "\n",
    "v_size = len(tk.word_index)\n",
    "print(\"Vocabulary size: %s\" % v_size)\n",
    "\n",
    "X_train = tk.texts_to_sequences(train_texts_str)\n",
    "\n",
    "test_texts_str = [x.encode(\"utf8\") for x in test_texts]\n",
    "X_test = tk.texts_to_sequences(test_texts_str)\n",
    "\n",
    "print(text_to_word_sequence(train_texts_str[0]))\n",
    "print(X_train[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Transforming the labels\n",
    "\n",
    "* We transform the labels to binary vectors.\n",
    "* Keras has a convenience function to do this ([keras.np_utils.to_categorical](https://keras.io/utils/np_utils/)), but\n",
    "* Sklearn's [MultiLabelBinarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) has some useful extra methods, e.g. reverse transforming the binary vector to labels when doing the prediction step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform([(l, ) for l in train_labels])\n",
    "y_test = mlb.transform([(l, ) for l in test_labels])\n",
    "\n",
    "nb_classes = len(mlb.classes_)\n",
    "\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Padding the input sequences\n",
    "\n",
    "* To do Matrix multiplications each sample in a batch needs the same length, so we pad with 0\n",
    "* In the Tokenizer 0 is reserved and will not be given to a \"normal\" word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen: 16333\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "maxlen = max(len(x) for x in X_train)\n",
    "print(\"maxlen: %s\" % maxlen) \n",
    "\n",
    "maxlen = 1000  # for faster training max 1000 words\n",
    "\n",
    "X_train_padded = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test_padded = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# Because in our Model we just average over the word vectors\n",
    "# we do not want the zero vectors from the padding in the average,\n",
    "# so we prepare a special layer that removes those.\n",
    "# From: https://github.com/fchollet/keras/issues/2728\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class ZeroMaskedEntries(Layer):\n",
    "    \"\"\"\n",
    "    This layer is called after an Embedding layer.\n",
    "    It zeros out all of the masked-out embeddings.\n",
    "    It also swallows the mask without passing it on.\n",
    "    You can change this to default pass-on behavior as follows:\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return K.not_equal(x, 0)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.support_mask = True\n",
    "        super(ZeroMaskedEntries, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[1]\n",
    "        self.repeat_dim = input_shape[2]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        mask = K.cast(mask, 'float32')\n",
    "        mask = K.repeat(mask, self.repeat_dim)\n",
    "        mask = K.permute_dimensions(mask, (0, 2, 1))\n",
    "        return x * mask\n",
    "\n",
    "    def compute_mask(self, input_shape, input_mask=None):\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_15 (Embedding)         (None, None, 100)     13414300    embedding_input_15[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "zeromaskedentries_15 (ZeroMasked (None, None, 100)     0           embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling1d_15 (Globa (None, 100)           0           zeromaskedentries_15[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 100)           0           globalaveragepooling1d_15[0][0]  \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 20)            2020        dropout_11[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 13,416,320\n",
      "Trainable params: 13,416,320\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# modified version of https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.regularizers import l2\n",
    "\n",
    "embedding_dims = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "emb = Embedding(v_size + 1, embedding_dims,\n",
    "                mask_zero=True,  # Mask out padded tokens\n",
    "                dropout=0.2,\n",
    "               )\n",
    "model.add(emb)\n",
    "\n",
    "# Add special layer because averaging layer does not support mask_zero=True\n",
    "model.add(ZeroMaskedEntries())  \n",
    "\n",
    "# averaging layer\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(nb_classes, activation='sigmoid', W_regularizer=l2(0.00001)))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'precision', 'recall', 'fmeasure'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training the model\n",
    "\n",
    "* If your data is too big to fit in memory use [fit_generator](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/500\n",
      "8s - loss: 0.4521 - acc: 0.9480 - precision: 6.5776e-04 - recall: 0.0020 - fmeasure: 7.3241e-04 - val_loss: 0.2932 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 2/500\n",
      "7s - loss: 0.2732 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2635 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 3/500\n",
      "7s - loss: 0.2559 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2508 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 4/500\n",
      "7s - loss: 0.2435 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2393 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 5/500\n",
      "7s - loss: 0.2319 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2294 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 6/500\n",
      "7s - loss: 0.2231 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2216 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 7/500\n",
      "7s - loss: 0.2161 - acc: 0.9500 - precision: 0.0000e+00 - recall: 0.0000e+00 - fmeasure: 0.0000e+00 - val_loss: 0.2157 - val_acc: 0.9500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_fmeasure: 0.0000e+00\n",
      "Epoch 8/500\n",
      "7s - loss: 0.2107 - acc: 0.9500 - precision: 0.0071 - recall: 2.6516e-04 - fmeasure: 5.0920e-04 - val_loss: 0.2106 - val_acc: 0.9500 - val_precision: 0.0085 - val_recall: 2.6553e-04 - val_fmeasure: 5.1497e-04\n",
      "Epoch 9/500\n",
      "7s - loss: 0.2057 - acc: 0.9500 - precision: 0.0594 - recall: 0.0019 - fmeasure: 0.0038 - val_loss: 0.2060 - val_acc: 0.9500 - val_precision: 0.0467 - val_recall: 0.0015 - val_fmeasure: 0.0028\n",
      "Epoch 10/500\n",
      "7s - loss: 0.2006 - acc: 0.9501 - precision: 0.1118 - recall: 0.0044 - fmeasure: 0.0084 - val_loss: 0.2014 - val_acc: 0.9501 - val_precision: 0.1020 - val_recall: 0.0036 - val_fmeasure: 0.0069\n",
      "Epoch 11/500\n",
      "7s - loss: 0.1954 - acc: 0.9502 - precision: 0.1905 - recall: 0.0078 - fmeasure: 0.0148 - val_loss: 0.1966 - val_acc: 0.9502 - val_precision: 0.1593 - val_recall: 0.0054 - val_fmeasure: 0.0105\n",
      "Epoch 12/500\n",
      "7s - loss: 0.1894 - acc: 0.9505 - precision: 0.3162 - recall: 0.0136 - fmeasure: 0.0259 - val_loss: 0.1915 - val_acc: 0.9504 - val_precision: 0.2514 - val_recall: 0.0092 - val_fmeasure: 0.0176\n",
      "Epoch 13/500\n",
      "7s - loss: 0.1828 - acc: 0.9508 - precision: 0.4258 - recall: 0.0203 - fmeasure: 0.0384 - val_loss: 0.1857 - val_acc: 0.9505 - val_precision: 0.3349 - val_recall: 0.0139 - val_fmeasure: 0.0266\n",
      "Epoch 14/500\n",
      "7s - loss: 0.1756 - acc: 0.9512 - precision: 0.5850 - recall: 0.0288 - fmeasure: 0.0543 - val_loss: 0.1800 - val_acc: 0.9507 - val_precision: 0.4285 - val_recall: 0.0210 - val_fmeasure: 0.0395\n",
      "Epoch 15/500\n",
      "7s - loss: 0.1687 - acc: 0.9519 - precision: 0.7012 - recall: 0.0454 - fmeasure: 0.0839 - val_loss: 0.1748 - val_acc: 0.9509 - val_precision: 0.4768 - val_recall: 0.0242 - val_fmeasure: 0.0454\n",
      "Epoch 16/500\n",
      "7s - loss: 0.1618 - acc: 0.9526 - precision: 0.7860 - recall: 0.0597 - fmeasure: 0.1087 - val_loss: 0.1696 - val_acc: 0.9513 - val_precision: 0.5886 - val_recall: 0.0351 - val_fmeasure: 0.0649\n",
      "Epoch 17/500\n",
      "7s - loss: 0.1556 - acc: 0.9536 - precision: 0.8517 - recall: 0.0814 - fmeasure: 0.1449 - val_loss: 0.1650 - val_acc: 0.9518 - val_precision: 0.6736 - val_recall: 0.0445 - val_fmeasure: 0.0818\n",
      "Epoch 18/500\n",
      "7s - loss: 0.1496 - acc: 0.9547 - precision: 0.9065 - recall: 0.1033 - fmeasure: 0.1814 - val_loss: 0.1607 - val_acc: 0.9523 - val_precision: 0.7366 - val_recall: 0.0564 - val_fmeasure: 0.1027\n",
      "Epoch 19/500\n",
      "7s - loss: 0.1442 - acc: 0.9558 - precision: 0.9338 - recall: 0.1267 - fmeasure: 0.2178 - val_loss: 0.1569 - val_acc: 0.9529 - val_precision: 0.7939 - val_recall: 0.0693 - val_fmeasure: 0.1246\n",
      "Epoch 20/500\n",
      "7s - loss: 0.1389 - acc: 0.9574 - precision: 0.9448 - recall: 0.1576 - fmeasure: 0.2644 - val_loss: 0.1535 - val_acc: 0.9536 - val_precision: 0.8377 - val_recall: 0.0843 - val_fmeasure: 0.1495\n",
      "Epoch 21/500\n",
      "7s - loss: 0.1343 - acc: 0.9587 - precision: 0.9479 - recall: 0.1842 - fmeasure: 0.3023 - val_loss: 0.1503 - val_acc: 0.9544 - val_precision: 0.8739 - val_recall: 0.1021 - val_fmeasure: 0.1782\n",
      "Epoch 22/500\n",
      "7s - loss: 0.1300 - acc: 0.9603 - precision: 0.9585 - recall: 0.2168 - fmeasure: 0.3477 - val_loss: 0.1474 - val_acc: 0.9550 - val_precision: 0.8871 - val_recall: 0.1162 - val_fmeasure: 0.2003\n",
      "Epoch 23/500\n",
      "7s - loss: 0.1259 - acc: 0.9615 - precision: 0.9557 - recall: 0.2416 - fmeasure: 0.3804 - val_loss: 0.1448 - val_acc: 0.9556 - val_precision: 0.8939 - val_recall: 0.1294 - val_fmeasure: 0.2208\n",
      "Epoch 24/500\n",
      "7s - loss: 0.1223 - acc: 0.9628 - precision: 0.9617 - recall: 0.2681 - fmeasure: 0.4129 - val_loss: 0.1424 - val_acc: 0.9564 - val_precision: 0.9003 - val_recall: 0.1446 - val_fmeasure: 0.2432\n",
      "Epoch 25/500\n",
      "7s - loss: 0.1190 - acc: 0.9642 - precision: 0.9690 - recall: 0.2934 - fmeasure: 0.4440 - val_loss: 0.1401 - val_acc: 0.9572 - val_precision: 0.9049 - val_recall: 0.1626 - val_fmeasure: 0.2696\n",
      "Epoch 26/500\n",
      "7s - loss: 0.1158 - acc: 0.9652 - precision: 0.9716 - recall: 0.3146 - fmeasure: 0.4680 - val_loss: 0.1380 - val_acc: 0.9580 - val_precision: 0.9105 - val_recall: 0.1790 - val_fmeasure: 0.2924\n",
      "Epoch 27/500\n",
      "7s - loss: 0.1129 - acc: 0.9665 - precision: 0.9726 - recall: 0.3401 - fmeasure: 0.4978 - val_loss: 0.1363 - val_acc: 0.9584 - val_precision: 0.9196 - val_recall: 0.1876 - val_fmeasure: 0.3047\n",
      "Epoch 28/500\n",
      "7s - loss: 0.1098 - acc: 0.9678 - precision: 0.9755 - recall: 0.3664 - fmeasure: 0.5257 - val_loss: 0.1346 - val_acc: 0.9588 - val_precision: 0.9214 - val_recall: 0.1949 - val_fmeasure: 0.3146\n",
      "Epoch 29/500\n",
      "7s - loss: 0.1073 - acc: 0.9687 - precision: 0.9775 - recall: 0.3826 - fmeasure: 0.5439 - val_loss: 0.1327 - val_acc: 0.9596 - val_precision: 0.9208 - val_recall: 0.2132 - val_fmeasure: 0.3390\n",
      "Epoch 30/500\n",
      "7s - loss: 0.1051 - acc: 0.9696 - precision: 0.9727 - recall: 0.4041 - fmeasure: 0.5651 - val_loss: 0.1312 - val_acc: 0.9603 - val_precision: 0.9283 - val_recall: 0.2244 - val_fmeasure: 0.3544\n",
      "Epoch 31/500\n",
      "7s - loss: 0.1029 - acc: 0.9703 - precision: 0.9789 - recall: 0.4157 - fmeasure: 0.5778 - val_loss: 0.1299 - val_acc: 0.9607 - val_precision: 0.9306 - val_recall: 0.2323 - val_fmeasure: 0.3649\n",
      "Epoch 32/500\n",
      "7s - loss: 0.1007 - acc: 0.9716 - precision: 0.9791 - recall: 0.4415 - fmeasure: 0.6035 - val_loss: 0.1283 - val_acc: 0.9613 - val_precision: 0.9320 - val_recall: 0.2460 - val_fmeasure: 0.3823\n",
      "Epoch 33/500\n",
      "7s - loss: 0.0984 - acc: 0.9724 - precision: 0.9799 - recall: 0.4585 - fmeasure: 0.6187 - val_loss: 0.1272 - val_acc: 0.9616 - val_precision: 0.9337 - val_recall: 0.2507 - val_fmeasure: 0.3884\n",
      "Epoch 34/500\n",
      "7s - loss: 0.0963 - acc: 0.9733 - precision: 0.9835 - recall: 0.4743 - fmeasure: 0.6347 - val_loss: 0.1259 - val_acc: 0.9620 - val_precision: 0.9334 - val_recall: 0.2605 - val_fmeasure: 0.4005\n",
      "Epoch 35/500\n",
      "7s - loss: 0.0944 - acc: 0.9740 - precision: 0.9818 - recall: 0.4890 - fmeasure: 0.6475 - val_loss: 0.1246 - val_acc: 0.9624 - val_precision: 0.9352 - val_recall: 0.2687 - val_fmeasure: 0.4109\n",
      "Epoch 36/500\n",
      "7s - loss: 0.0927 - acc: 0.9750 - precision: 0.9820 - recall: 0.5088 - fmeasure: 0.6658 - val_loss: 0.1234 - val_acc: 0.9629 - val_precision: 0.9363 - val_recall: 0.2784 - val_fmeasure: 0.4226\n",
      "Epoch 37/500\n",
      "7s - loss: 0.0914 - acc: 0.9755 - precision: 0.9831 - recall: 0.5186 - fmeasure: 0.6739 - val_loss: 0.1224 - val_acc: 0.9632 - val_precision: 0.9384 - val_recall: 0.2843 - val_fmeasure: 0.4298\n",
      "Epoch 38/500\n",
      "7s - loss: 0.0893 - acc: 0.9764 - precision: 0.9844 - recall: 0.5371 - fmeasure: 0.6907 - val_loss: 0.1214 - val_acc: 0.9635 - val_precision: 0.9395 - val_recall: 0.2889 - val_fmeasure: 0.4355\n",
      "Epoch 39/500\n",
      "7s - loss: 0.0880 - acc: 0.9769 - precision: 0.9853 - recall: 0.5463 - fmeasure: 0.6983 - val_loss: 0.1204 - val_acc: 0.9638 - val_precision: 0.9405 - val_recall: 0.2953 - val_fmeasure: 0.4430\n",
      "Epoch 40/500\n",
      "7s - loss: 0.0864 - acc: 0.9774 - precision: 0.9831 - recall: 0.5589 - fmeasure: 0.7080 - val_loss: 0.1192 - val_acc: 0.9641 - val_precision: 0.9399 - val_recall: 0.3032 - val_fmeasure: 0.4521\n",
      "Epoch 41/500\n",
      "7s - loss: 0.0849 - acc: 0.9782 - precision: 0.9858 - recall: 0.5734 - fmeasure: 0.7211 - val_loss: 0.1183 - val_acc: 0.9644 - val_precision: 0.9412 - val_recall: 0.3092 - val_fmeasure: 0.4592\n",
      "Epoch 42/500\n",
      "7s - loss: 0.0836 - acc: 0.9786 - precision: 0.9852 - recall: 0.5809 - fmeasure: 0.7265 - val_loss: 0.1175 - val_acc: 0.9646 - val_precision: 0.9429 - val_recall: 0.3129 - val_fmeasure: 0.4638\n",
      "Epoch 43/500\n",
      "7s - loss: 0.0822 - acc: 0.9793 - precision: 0.9857 - recall: 0.5952 - fmeasure: 0.7386 - val_loss: 0.1168 - val_acc: 0.9649 - val_precision: 0.9395 - val_recall: 0.3202 - val_fmeasure: 0.4712\n",
      "Epoch 44/500\n",
      "7s - loss: 0.0810 - acc: 0.9796 - precision: 0.9873 - recall: 0.6008 - fmeasure: 0.7434 - val_loss: 0.1158 - val_acc: 0.9652 - val_precision: 0.9401 - val_recall: 0.3267 - val_fmeasure: 0.4787\n",
      "Epoch 45/500\n",
      "7s - loss: 0.0799 - acc: 0.9802 - precision: 0.9855 - recall: 0.6128 - fmeasure: 0.7516 - val_loss: 0.1151 - val_acc: 0.9655 - val_precision: 0.9412 - val_recall: 0.3310 - val_fmeasure: 0.4836\n",
      "Epoch 46/500\n",
      "7s - loss: 0.0785 - acc: 0.9807 - precision: 0.9887 - recall: 0.6214 - fmeasure: 0.7596 - val_loss: 0.1143 - val_acc: 0.9657 - val_precision: 0.9409 - val_recall: 0.3370 - val_fmeasure: 0.4898\n",
      "Epoch 47/500\n",
      "7s - loss: 0.0772 - acc: 0.9814 - precision: 0.9898 - recall: 0.6357 - fmeasure: 0.7706 - val_loss: 0.1137 - val_acc: 0.9658 - val_precision: 0.9421 - val_recall: 0.3379 - val_fmeasure: 0.4912\n",
      "Epoch 48/500\n",
      "7s - loss: 0.0762 - acc: 0.9814 - precision: 0.9879 - recall: 0.6358 - fmeasure: 0.7695 - val_loss: 0.1127 - val_acc: 0.9662 - val_precision: 0.9417 - val_recall: 0.3461 - val_fmeasure: 0.5001\n",
      "Epoch 49/500\n",
      "7s - loss: 0.0751 - acc: 0.9820 - precision: 0.9888 - recall: 0.6482 - fmeasure: 0.7793 - val_loss: 0.1121 - val_acc: 0.9665 - val_precision: 0.9425 - val_recall: 0.3525 - val_fmeasure: 0.5071\n",
      "Epoch 50/500\n",
      "7s - loss: 0.0742 - acc: 0.9825 - precision: 0.9893 - recall: 0.6568 - fmeasure: 0.7860 - val_loss: 0.1113 - val_acc: 0.9668 - val_precision: 0.9432 - val_recall: 0.3579 - val_fmeasure: 0.5129\n",
      "Epoch 51/500\n",
      "8s - loss: 0.0736 - acc: 0.9824 - precision: 0.9889 - recall: 0.6551 - fmeasure: 0.7848 - val_loss: 0.1107 - val_acc: 0.9670 - val_precision: 0.9441 - val_recall: 0.3615 - val_fmeasure: 0.5168\n",
      "Epoch 52/500\n",
      "7s - loss: 0.0723 - acc: 0.9829 - precision: 0.9873 - recall: 0.6675 - fmeasure: 0.7933 - val_loss: 0.1100 - val_acc: 0.9673 - val_precision: 0.9443 - val_recall: 0.3676 - val_fmeasure: 0.5233\n",
      "Epoch 53/500\n",
      "7s - loss: 0.0715 - acc: 0.9832 - precision: 0.9890 - recall: 0.6720 - fmeasure: 0.7971 - val_loss: 0.1095 - val_acc: 0.9674 - val_precision: 0.9427 - val_recall: 0.3713 - val_fmeasure: 0.5271\n",
      "Epoch 54/500\n",
      "7s - loss: 0.0701 - acc: 0.9839 - precision: 0.9905 - recall: 0.6857 - fmeasure: 0.8075 - val_loss: 0.1088 - val_acc: 0.9676 - val_precision: 0.9458 - val_recall: 0.3740 - val_fmeasure: 0.5301\n",
      "Epoch 55/500\n",
      "7s - loss: 0.0696 - acc: 0.9841 - precision: 0.9887 - recall: 0.6892 - fmeasure: 0.8092 - val_loss: 0.1082 - val_acc: 0.9679 - val_precision: 0.9444 - val_recall: 0.3804 - val_fmeasure: 0.5365\n",
      "Epoch 56/500\n",
      "7s - loss: 0.0683 - acc: 0.9846 - precision: 0.9898 - recall: 0.6995 - fmeasure: 0.8167 - val_loss: 0.1078 - val_acc: 0.9680 - val_precision: 0.9443 - val_recall: 0.3824 - val_fmeasure: 0.5385\n",
      "Epoch 57/500\n",
      "7s - loss: 0.0679 - acc: 0.9847 - precision: 0.9905 - recall: 0.7005 - fmeasure: 0.8183 - val_loss: 0.1073 - val_acc: 0.9681 - val_precision: 0.9459 - val_recall: 0.3844 - val_fmeasure: 0.5407\n",
      "Epoch 58/500\n",
      "7s - loss: 0.0669 - acc: 0.9851 - precision: 0.9907 - recall: 0.7098 - fmeasure: 0.8243 - val_loss: 0.1066 - val_acc: 0.9682 - val_precision: 0.9464 - val_recall: 0.3867 - val_fmeasure: 0.5433\n",
      "Epoch 59/500\n",
      "7s - loss: 0.0662 - acc: 0.9852 - precision: 0.9905 - recall: 0.7100 - fmeasure: 0.8244 - val_loss: 0.1060 - val_acc: 0.9685 - val_precision: 0.9443 - val_recall: 0.3931 - val_fmeasure: 0.5493\n",
      "Epoch 60/500\n",
      "7s - loss: 0.0653 - acc: 0.9858 - precision: 0.9911 - recall: 0.7223 - fmeasure: 0.8328 - val_loss: 0.1055 - val_acc: 0.9686 - val_precision: 0.9426 - val_recall: 0.3971 - val_fmeasure: 0.5529\n",
      "Epoch 61/500\n",
      "7s - loss: 0.0646 - acc: 0.9858 - precision: 0.9908 - recall: 0.7223 - fmeasure: 0.8329 - val_loss: 0.1049 - val_acc: 0.9688 - val_precision: 0.9444 - val_recall: 0.3998 - val_fmeasure: 0.5558\n",
      "Epoch 62/500\n",
      "7s - loss: 0.0638 - acc: 0.9861 - precision: 0.9912 - recall: 0.7284 - fmeasure: 0.8375 - val_loss: 0.1046 - val_acc: 0.9688 - val_precision: 0.9453 - val_recall: 0.3994 - val_fmeasure: 0.5556\n",
      "Epoch 63/500\n",
      "7s - loss: 0.0632 - acc: 0.9862 - precision: 0.9908 - recall: 0.7310 - fmeasure: 0.8388 - val_loss: 0.1041 - val_acc: 0.9689 - val_precision: 0.9464 - val_recall: 0.4025 - val_fmeasure: 0.5588\n",
      "Epoch 64/500\n",
      "7s - loss: 0.0627 - acc: 0.9863 - precision: 0.9909 - recall: 0.7335 - fmeasure: 0.8408 - val_loss: 0.1035 - val_acc: 0.9691 - val_precision: 0.9453 - val_recall: 0.4067 - val_fmeasure: 0.5627\n",
      "Epoch 65/500\n",
      "7s - loss: 0.0619 - acc: 0.9868 - precision: 0.9915 - recall: 0.7424 - fmeasure: 0.8466 - val_loss: 0.1032 - val_acc: 0.9692 - val_precision: 0.9460 - val_recall: 0.4077 - val_fmeasure: 0.5640\n",
      "Epoch 66/500\n",
      "7s - loss: 0.0612 - acc: 0.9869 - precision: 0.9901 - recall: 0.7447 - fmeasure: 0.8476 - val_loss: 0.1028 - val_acc: 0.9694 - val_precision: 0.9490 - val_recall: 0.4116 - val_fmeasure: 0.5684\n",
      "Epoch 67/500\n",
      "7s - loss: 0.0605 - acc: 0.9873 - precision: 0.9920 - recall: 0.7523 - fmeasure: 0.8535 - val_loss: 0.1024 - val_acc: 0.9694 - val_precision: 0.9475 - val_recall: 0.4109 - val_fmeasure: 0.5676\n",
      "Epoch 68/500\n",
      "7s - loss: 0.0599 - acc: 0.9872 - precision: 0.9910 - recall: 0.7512 - fmeasure: 0.8524 - val_loss: 0.1019 - val_acc: 0.9697 - val_precision: 0.9490 - val_recall: 0.4172 - val_fmeasure: 0.5741\n",
      "Epoch 69/500\n",
      "7s - loss: 0.0592 - acc: 0.9877 - precision: 0.9924 - recall: 0.7607 - fmeasure: 0.8590 - val_loss: 0.1017 - val_acc: 0.9696 - val_precision: 0.9468 - val_recall: 0.4168 - val_fmeasure: 0.5732\n",
      "Epoch 70/500\n",
      "7s - loss: 0.0587 - acc: 0.9879 - precision: 0.9913 - recall: 0.7639 - fmeasure: 0.8609 - val_loss: 0.1010 - val_acc: 0.9700 - val_precision: 0.9458 - val_recall: 0.4243 - val_fmeasure: 0.5803\n",
      "Epoch 71/500\n",
      "8s - loss: 0.0580 - acc: 0.9882 - precision: 0.9933 - recall: 0.7683 - fmeasure: 0.8645 - val_loss: 0.1005 - val_acc: 0.9700 - val_precision: 0.9456 - val_recall: 0.4262 - val_fmeasure: 0.5821\n",
      "Epoch 72/500\n",
      "7s - loss: 0.0571 - acc: 0.9885 - precision: 0.9933 - recall: 0.7754 - fmeasure: 0.8689 - val_loss: 0.1004 - val_acc: 0.9700 - val_precision: 0.9460 - val_recall: 0.4251 - val_fmeasure: 0.5810\n",
      "Epoch 73/500\n",
      "7s - loss: 0.0568 - acc: 0.9883 - precision: 0.9918 - recall: 0.7735 - fmeasure: 0.8672 - val_loss: 0.1001 - val_acc: 0.9702 - val_precision: 0.9474 - val_recall: 0.4282 - val_fmeasure: 0.5844\n",
      "Epoch 74/500\n",
      "7s - loss: 0.0564 - acc: 0.9884 - precision: 0.9923 - recall: 0.7748 - fmeasure: 0.8679 - val_loss: 0.0994 - val_acc: 0.9704 - val_precision: 0.9465 - val_recall: 0.4343 - val_fmeasure: 0.5901\n",
      "Epoch 75/500\n",
      "7s - loss: 0.0556 - acc: 0.9888 - precision: 0.9915 - recall: 0.7828 - fmeasure: 0.8730 - val_loss: 0.0993 - val_acc: 0.9704 - val_precision: 0.9489 - val_recall: 0.4326 - val_fmeasure: 0.5889\n",
      "Epoch 76/500\n",
      "7s - loss: 0.0553 - acc: 0.9889 - precision: 0.9940 - recall: 0.7823 - fmeasure: 0.8733 - val_loss: 0.0987 - val_acc: 0.9706 - val_precision: 0.9486 - val_recall: 0.4364 - val_fmeasure: 0.5924\n",
      "Epoch 77/500\n",
      "7s - loss: 0.0547 - acc: 0.9890 - precision: 0.9930 - recall: 0.7861 - fmeasure: 0.8758 - val_loss: 0.0987 - val_acc: 0.9706 - val_precision: 0.9474 - val_recall: 0.4365 - val_fmeasure: 0.5923\n",
      "Epoch 78/500\n",
      "7s - loss: 0.0543 - acc: 0.9892 - precision: 0.9924 - recall: 0.7902 - fmeasure: 0.8780 - val_loss: 0.0981 - val_acc: 0.9707 - val_precision: 0.9481 - val_recall: 0.4387 - val_fmeasure: 0.5945\n",
      "Epoch 79/500\n",
      "7s - loss: 0.0537 - acc: 0.9896 - precision: 0.9938 - recall: 0.7965 - fmeasure: 0.8824 - val_loss: 0.0980 - val_acc: 0.9708 - val_precision: 0.9468 - val_recall: 0.4408 - val_fmeasure: 0.5962\n",
      "Epoch 80/500\n",
      "7s - loss: 0.0533 - acc: 0.9897 - precision: 0.9929 - recall: 0.7989 - fmeasure: 0.8834 - val_loss: 0.0975 - val_acc: 0.9710 - val_precision: 0.9484 - val_recall: 0.4448 - val_fmeasure: 0.6002\n",
      "Epoch 81/500\n",
      "7s - loss: 0.0530 - acc: 0.9897 - precision: 0.9940 - recall: 0.7994 - fmeasure: 0.8843 - val_loss: 0.0969 - val_acc: 0.9712 - val_precision: 0.9460 - val_recall: 0.4507 - val_fmeasure: 0.6052\n",
      "Epoch 82/500\n",
      "7s - loss: 0.0523 - acc: 0.9900 - precision: 0.9936 - recall: 0.8045 - fmeasure: 0.8873 - val_loss: 0.0969 - val_acc: 0.9712 - val_precision: 0.9472 - val_recall: 0.4492 - val_fmeasure: 0.6041\n",
      "Epoch 83/500\n",
      "7s - loss: 0.0518 - acc: 0.9901 - precision: 0.9938 - recall: 0.8078 - fmeasure: 0.8894 - val_loss: 0.0966 - val_acc: 0.9712 - val_precision: 0.9481 - val_recall: 0.4486 - val_fmeasure: 0.6037\n",
      "Epoch 84/500\n",
      "7s - loss: 0.0513 - acc: 0.9902 - precision: 0.9939 - recall: 0.8101 - fmeasure: 0.8909 - val_loss: 0.0963 - val_acc: 0.9713 - val_precision: 0.9483 - val_recall: 0.4514 - val_fmeasure: 0.6064\n",
      "Epoch 85/500\n",
      "7s - loss: 0.0507 - acc: 0.9903 - precision: 0.9924 - recall: 0.8133 - fmeasure: 0.8924 - val_loss: 0.0961 - val_acc: 0.9713 - val_precision: 0.9492 - val_recall: 0.4506 - val_fmeasure: 0.6059\n",
      "Epoch 86/500\n",
      "7s - loss: 0.0504 - acc: 0.9905 - precision: 0.9943 - recall: 0.8146 - fmeasure: 0.8939 - val_loss: 0.0958 - val_acc: 0.9716 - val_precision: 0.9487 - val_recall: 0.4569 - val_fmeasure: 0.6115\n",
      "Epoch 87/500\n",
      "7s - loss: 0.0498 - acc: 0.9907 - precision: 0.9945 - recall: 0.8180 - fmeasure: 0.8960 - val_loss: 0.0957 - val_acc: 0.9715 - val_precision: 0.9479 - val_recall: 0.4559 - val_fmeasure: 0.6105\n",
      "Epoch 88/500\n",
      "7s - loss: 0.0495 - acc: 0.9908 - precision: 0.9942 - recall: 0.8208 - fmeasure: 0.8975 - val_loss: 0.0953 - val_acc: 0.9715 - val_precision: 0.9491 - val_recall: 0.4562 - val_fmeasure: 0.6108\n",
      "Epoch 89/500\n",
      "7s - loss: 0.0490 - acc: 0.9910 - precision: 0.9943 - recall: 0.8253 - fmeasure: 0.9005 - val_loss: 0.0951 - val_acc: 0.9716 - val_precision: 0.9494 - val_recall: 0.4575 - val_fmeasure: 0.6122\n",
      "Epoch 90/500\n",
      "7s - loss: 0.0488 - acc: 0.9910 - precision: 0.9937 - recall: 0.8253 - fmeasure: 0.9001 - val_loss: 0.0947 - val_acc: 0.9717 - val_precision: 0.9488 - val_recall: 0.4602 - val_fmeasure: 0.6145\n",
      "Epoch 91/500\n",
      "7s - loss: 0.0483 - acc: 0.9911 - precision: 0.9945 - recall: 0.8273 - fmeasure: 0.9019 - val_loss: 0.0946 - val_acc: 0.9718 - val_precision: 0.9497 - val_recall: 0.4611 - val_fmeasure: 0.6155\n",
      "Epoch 92/500\n",
      "7s - loss: 0.0481 - acc: 0.9911 - precision: 0.9948 - recall: 0.8270 - fmeasure: 0.9016 - val_loss: 0.0943 - val_acc: 0.9718 - val_precision: 0.9496 - val_recall: 0.4622 - val_fmeasure: 0.6166\n",
      "Epoch 93/500\n",
      "7s - loss: 0.0474 - acc: 0.9914 - precision: 0.9944 - recall: 0.8322 - fmeasure: 0.9045 - val_loss: 0.0943 - val_acc: 0.9719 - val_precision: 0.9494 - val_recall: 0.4639 - val_fmeasure: 0.6181\n",
      "Epoch 94/500\n",
      "7s - loss: 0.0472 - acc: 0.9914 - precision: 0.9943 - recall: 0.8329 - fmeasure: 0.9051 - val_loss: 0.0937 - val_acc: 0.9720 - val_precision: 0.9496 - val_recall: 0.4663 - val_fmeasure: 0.6204\n",
      "Epoch 95/500\n",
      "7s - loss: 0.0470 - acc: 0.9914 - precision: 0.9943 - recall: 0.8334 - fmeasure: 0.9055 - val_loss: 0.0934 - val_acc: 0.9722 - val_precision: 0.9490 - val_recall: 0.4696 - val_fmeasure: 0.6232\n",
      "Epoch 96/500\n",
      "7s - loss: 0.0466 - acc: 0.9915 - precision: 0.9935 - recall: 0.8345 - fmeasure: 0.9054 - val_loss: 0.0930 - val_acc: 0.9723 - val_precision: 0.9483 - val_recall: 0.4729 - val_fmeasure: 0.6261\n",
      "Epoch 97/500\n",
      "7s - loss: 0.0461 - acc: 0.9919 - precision: 0.9942 - recall: 0.8428 - fmeasure: 0.9108 - val_loss: 0.0931 - val_acc: 0.9721 - val_precision: 0.9498 - val_recall: 0.4683 - val_fmeasure: 0.6222\n",
      "Epoch 98/500\n",
      "7s - loss: 0.0456 - acc: 0.9919 - precision: 0.9949 - recall: 0.8422 - fmeasure: 0.9110 - val_loss: 0.0928 - val_acc: 0.9724 - val_precision: 0.9493 - val_recall: 0.4741 - val_fmeasure: 0.6274\n",
      "Epoch 99/500\n",
      "7s - loss: 0.0454 - acc: 0.9919 - precision: 0.9957 - recall: 0.8426 - fmeasure: 0.9116 - val_loss: 0.0926 - val_acc: 0.9723 - val_precision: 0.9480 - val_recall: 0.4736 - val_fmeasure: 0.6266\n",
      "Epoch 100/500\n",
      "7s - loss: 0.0452 - acc: 0.9921 - precision: 0.9942 - recall: 0.8477 - fmeasure: 0.9139 - val_loss: 0.0924 - val_acc: 0.9725 - val_precision: 0.9494 - val_recall: 0.4758 - val_fmeasure: 0.6289\n",
      "Epoch 101/500\n",
      "7s - loss: 0.0447 - acc: 0.9921 - precision: 0.9959 - recall: 0.8462 - fmeasure: 0.9135 - val_loss: 0.0923 - val_acc: 0.9726 - val_precision: 0.9496 - val_recall: 0.4776 - val_fmeasure: 0.6306\n",
      "Epoch 102/500\n",
      "7s - loss: 0.0444 - acc: 0.9922 - precision: 0.9944 - recall: 0.8479 - fmeasure: 0.9140 - val_loss: 0.0920 - val_acc: 0.9726 - val_precision: 0.9499 - val_recall: 0.4774 - val_fmeasure: 0.6307\n",
      "Epoch 103/500\n",
      "7s - loss: 0.0442 - acc: 0.9923 - precision: 0.9945 - recall: 0.8498 - fmeasure: 0.9152 - val_loss: 0.0919 - val_acc: 0.9726 - val_precision: 0.9501 - val_recall: 0.4770 - val_fmeasure: 0.6303\n",
      "Epoch 104/500\n",
      "7s - loss: 0.0438 - acc: 0.9924 - precision: 0.9952 - recall: 0.8512 - fmeasure: 0.9164 - val_loss: 0.0914 - val_acc: 0.9728 - val_precision: 0.9512 - val_recall: 0.4814 - val_fmeasure: 0.6345\n",
      "Epoch 105/500\n",
      "7s - loss: 0.0435 - acc: 0.9926 - precision: 0.9955 - recall: 0.8558 - fmeasure: 0.9190 - val_loss: 0.0914 - val_acc: 0.9728 - val_precision: 0.9499 - val_recall: 0.4822 - val_fmeasure: 0.6349\n",
      "Epoch 106/500\n",
      "7s - loss: 0.0432 - acc: 0.9926 - precision: 0.9957 - recall: 0.8558 - fmeasure: 0.9192 - val_loss: 0.0911 - val_acc: 0.9729 - val_precision: 0.9505 - val_recall: 0.4841 - val_fmeasure: 0.6368\n",
      "Epoch 107/500\n",
      "7s - loss: 0.0431 - acc: 0.9926 - precision: 0.9950 - recall: 0.8565 - fmeasure: 0.9193 - val_loss: 0.0909 - val_acc: 0.9730 - val_precision: 0.9512 - val_recall: 0.4853 - val_fmeasure: 0.6379\n",
      "Epoch 108/500\n",
      "7s - loss: 0.0427 - acc: 0.9929 - precision: 0.9958 - recall: 0.8611 - fmeasure: 0.9224 - val_loss: 0.0907 - val_acc: 0.9730 - val_precision: 0.9503 - val_recall: 0.4866 - val_fmeasure: 0.6389\n",
      "Epoch 109/500\n",
      "7s - loss: 0.0423 - acc: 0.9929 - precision: 0.9951 - recall: 0.8614 - fmeasure: 0.9223 - val_loss: 0.0906 - val_acc: 0.9731 - val_precision: 0.9493 - val_recall: 0.4894 - val_fmeasure: 0.6412\n",
      "Epoch 110/500\n",
      "7s - loss: 0.0421 - acc: 0.9929 - precision: 0.9951 - recall: 0.8627 - fmeasure: 0.9229 - val_loss: 0.0905 - val_acc: 0.9730 - val_precision: 0.9502 - val_recall: 0.4871 - val_fmeasure: 0.6394\n",
      "Epoch 111/500\n",
      "7s - loss: 0.0416 - acc: 0.9931 - precision: 0.9955 - recall: 0.8660 - fmeasure: 0.9251 - val_loss: 0.0902 - val_acc: 0.9730 - val_precision: 0.9494 - val_recall: 0.4878 - val_fmeasure: 0.6398\n",
      "Epoch 112/500\n",
      "7s - loss: 0.0414 - acc: 0.9931 - precision: 0.9950 - recall: 0.8660 - fmeasure: 0.9249 - val_loss: 0.0903 - val_acc: 0.9731 - val_precision: 0.9495 - val_recall: 0.4890 - val_fmeasure: 0.6409\n",
      "Epoch 113/500\n",
      "7s - loss: 0.0411 - acc: 0.9933 - precision: 0.9961 - recall: 0.8685 - fmeasure: 0.9268 - val_loss: 0.0901 - val_acc: 0.9732 - val_precision: 0.9517 - val_recall: 0.4906 - val_fmeasure: 0.6429\n",
      "Epoch 114/500\n",
      "7s - loss: 0.0410 - acc: 0.9934 - precision: 0.9949 - recall: 0.8719 - fmeasure: 0.9284 - val_loss: 0.0895 - val_acc: 0.9733 - val_precision: 0.9501 - val_recall: 0.4932 - val_fmeasure: 0.6448\n",
      "Epoch 115/500\n",
      "7s - loss: 0.0406 - acc: 0.9935 - precision: 0.9951 - recall: 0.8740 - fmeasure: 0.9295 - val_loss: 0.0899 - val_acc: 0.9732 - val_precision: 0.9491 - val_recall: 0.4906 - val_fmeasure: 0.6422\n",
      "Epoch 116/500\n",
      "7s - loss: 0.0402 - acc: 0.9934 - precision: 0.9957 - recall: 0.8722 - fmeasure: 0.9289 - val_loss: 0.0895 - val_acc: 0.9733 - val_precision: 0.9496 - val_recall: 0.4936 - val_fmeasure: 0.6449\n",
      "Epoch 117/500\n",
      "7s - loss: 0.0402 - acc: 0.9935 - precision: 0.9958 - recall: 0.8728 - fmeasure: 0.9291 - val_loss: 0.0893 - val_acc: 0.9733 - val_precision: 0.9501 - val_recall: 0.4928 - val_fmeasure: 0.6444\n",
      "Epoch 118/500\n",
      "7s - loss: 0.0398 - acc: 0.9935 - precision: 0.9951 - recall: 0.8749 - fmeasure: 0.9301 - val_loss: 0.0891 - val_acc: 0.9733 - val_precision: 0.9506 - val_recall: 0.4928 - val_fmeasure: 0.6445\n",
      "Epoch 119/500\n",
      "7s - loss: 0.0397 - acc: 0.9936 - precision: 0.9956 - recall: 0.8764 - fmeasure: 0.9311 - val_loss: 0.0888 - val_acc: 0.9736 - val_precision: 0.9505 - val_recall: 0.4984 - val_fmeasure: 0.6493\n",
      "Epoch 120/500\n",
      "7s - loss: 0.0392 - acc: 0.9937 - precision: 0.9964 - recall: 0.8776 - fmeasure: 0.9320 - val_loss: 0.0888 - val_acc: 0.9735 - val_precision: 0.9502 - val_recall: 0.4969 - val_fmeasure: 0.6481\n",
      "Epoch 121/500\n",
      "7s - loss: 0.0392 - acc: 0.9937 - precision: 0.9962 - recall: 0.8775 - fmeasure: 0.9320 - val_loss: 0.0887 - val_acc: 0.9735 - val_precision: 0.9500 - val_recall: 0.4976 - val_fmeasure: 0.6485\n",
      "Epoch 122/500\n",
      "7s - loss: 0.0391 - acc: 0.9938 - precision: 0.9957 - recall: 0.8791 - fmeasure: 0.9328 - val_loss: 0.0883 - val_acc: 0.9735 - val_precision: 0.9483 - val_recall: 0.4989 - val_fmeasure: 0.6493\n",
      "Epoch 123/500\n",
      "7s - loss: 0.0387 - acc: 0.9939 - precision: 0.9952 - recall: 0.8816 - fmeasure: 0.9339 - val_loss: 0.0882 - val_acc: 0.9737 - val_precision: 0.9509 - val_recall: 0.4997 - val_fmeasure: 0.6506\n",
      "Epoch 124/500\n",
      "7s - loss: 0.0383 - acc: 0.9941 - precision: 0.9962 - recall: 0.8857 - fmeasure: 0.9369 - val_loss: 0.0884 - val_acc: 0.9736 - val_precision: 0.9506 - val_recall: 0.4983 - val_fmeasure: 0.6492\n",
      "Epoch 125/500\n",
      "7s - loss: 0.0381 - acc: 0.9940 - precision: 0.9961 - recall: 0.8841 - fmeasure: 0.9357 - val_loss: 0.0882 - val_acc: 0.9736 - val_precision: 0.9501 - val_recall: 0.4997 - val_fmeasure: 0.6504\n",
      "Epoch 126/500\n",
      "7s - loss: 0.0378 - acc: 0.9942 - precision: 0.9975 - recall: 0.8854 - fmeasure: 0.9372 - val_loss: 0.0878 - val_acc: 0.9739 - val_precision: 0.9503 - val_recall: 0.5045 - val_fmeasure: 0.6547\n",
      "Epoch 127/500\n",
      "7s - loss: 0.0379 - acc: 0.9940 - precision: 0.9956 - recall: 0.8838 - fmeasure: 0.9354 - val_loss: 0.0880 - val_acc: 0.9737 - val_precision: 0.9503 - val_recall: 0.5015 - val_fmeasure: 0.6519\n",
      "Epoch 128/500\n",
      "7s - loss: 0.0375 - acc: 0.9942 - precision: 0.9961 - recall: 0.8884 - fmeasure: 0.9384 - val_loss: 0.0876 - val_acc: 0.9739 - val_precision: 0.9479 - val_recall: 0.5060 - val_fmeasure: 0.6554\n",
      "Epoch 129/500\n",
      "7s - loss: 0.0371 - acc: 0.9944 - precision: 0.9964 - recall: 0.8915 - fmeasure: 0.9402 - val_loss: 0.0878 - val_acc: 0.9738 - val_precision: 0.9489 - val_recall: 0.5035 - val_fmeasure: 0.6534\n",
      "Epoch 130/500\n",
      "7s - loss: 0.0372 - acc: 0.9943 - precision: 0.9953 - recall: 0.8903 - fmeasure: 0.9389 - val_loss: 0.0876 - val_acc: 0.9737 - val_precision: 0.9503 - val_recall: 0.5008 - val_fmeasure: 0.6514\n",
      "Epoch 131/500\n",
      "7s - loss: 0.0367 - acc: 0.9943 - precision: 0.9966 - recall: 0.8885 - fmeasure: 0.9385 - val_loss: 0.0872 - val_acc: 0.9739 - val_precision: 0.9491 - val_recall: 0.5065 - val_fmeasure: 0.6562\n",
      "Epoch 132/500\n",
      "7s - loss: 0.0365 - acc: 0.9945 - precision: 0.9964 - recall: 0.8924 - fmeasure: 0.9406 - val_loss: 0.0873 - val_acc: 0.9739 - val_precision: 0.9494 - val_recall: 0.5046 - val_fmeasure: 0.6546\n",
      "Epoch 133/500\n",
      "7s - loss: 0.0364 - acc: 0.9944 - precision: 0.9954 - recall: 0.8924 - fmeasure: 0.9403 - val_loss: 0.0873 - val_acc: 0.9739 - val_precision: 0.9498 - val_recall: 0.5062 - val_fmeasure: 0.6561\n",
      "Epoch 134/500\n",
      "7s - loss: 0.0363 - acc: 0.9946 - precision: 0.9959 - recall: 0.8952 - fmeasure: 0.9420 - val_loss: 0.0870 - val_acc: 0.9740 - val_precision: 0.9514 - val_recall: 0.5072 - val_fmeasure: 0.6573\n",
      "Epoch 135/500\n",
      "7s - loss: 0.0361 - acc: 0.9944 - precision: 0.9960 - recall: 0.8926 - fmeasure: 0.9405 - val_loss: 0.0868 - val_acc: 0.9740 - val_precision: 0.9499 - val_recall: 0.5069 - val_fmeasure: 0.6568\n",
      "Epoch 136/500\n",
      "7s - loss: 0.0357 - acc: 0.9947 - precision: 0.9971 - recall: 0.8958 - fmeasure: 0.9428 - val_loss: 0.0868 - val_acc: 0.9740 - val_precision: 0.9491 - val_recall: 0.5086 - val_fmeasure: 0.6581\n",
      "Epoch 137/500\n",
      "7s - loss: 0.0356 - acc: 0.9947 - precision: 0.9966 - recall: 0.8968 - fmeasure: 0.9431 - val_loss: 0.0867 - val_acc: 0.9741 - val_precision: 0.9496 - val_recall: 0.5088 - val_fmeasure: 0.6583\n",
      "Epoch 138/500\n",
      "7s - loss: 0.0353 - acc: 0.9948 - precision: 0.9960 - recall: 0.9005 - fmeasure: 0.9450 - val_loss: 0.0868 - val_acc: 0.9740 - val_precision: 0.9494 - val_recall: 0.5081 - val_fmeasure: 0.6577\n",
      "Epoch 139/500\n",
      "7s - loss: 0.0353 - acc: 0.9948 - precision: 0.9970 - recall: 0.8984 - fmeasure: 0.9444 - val_loss: 0.0865 - val_acc: 0.9742 - val_precision: 0.9484 - val_recall: 0.5116 - val_fmeasure: 0.6604\n",
      "Epoch 140/500\n",
      "7s - loss: 0.0349 - acc: 0.9948 - precision: 0.9967 - recall: 0.8992 - fmeasure: 0.9446 - val_loss: 0.0862 - val_acc: 0.9742 - val_precision: 0.9481 - val_recall: 0.5122 - val_fmeasure: 0.6609\n",
      "Epoch 141/500\n",
      "7s - loss: 0.0348 - acc: 0.9950 - precision: 0.9973 - recall: 0.9027 - fmeasure: 0.9468 - val_loss: 0.0860 - val_acc: 0.9743 - val_precision: 0.9494 - val_recall: 0.5133 - val_fmeasure: 0.6621\n",
      "Epoch 142/500\n",
      "7s - loss: 0.0346 - acc: 0.9949 - precision: 0.9956 - recall: 0.9018 - fmeasure: 0.9456 - val_loss: 0.0861 - val_acc: 0.9741 - val_precision: 0.9484 - val_recall: 0.5112 - val_fmeasure: 0.6601\n",
      "Epoch 143/500\n",
      "7s - loss: 0.0345 - acc: 0.9950 - precision: 0.9969 - recall: 0.9030 - fmeasure: 0.9469 - val_loss: 0.0860 - val_acc: 0.9742 - val_precision: 0.9497 - val_recall: 0.5119 - val_fmeasure: 0.6610\n",
      "Epoch 144/500\n",
      "7s - loss: 0.0343 - acc: 0.9950 - precision: 0.9962 - recall: 0.9044 - fmeasure: 0.9472 - val_loss: 0.0859 - val_acc: 0.9742 - val_precision: 0.9499 - val_recall: 0.5122 - val_fmeasure: 0.6614\n",
      "Epoch 145/500\n",
      "7s - loss: 0.0340 - acc: 0.9953 - precision: 0.9975 - recall: 0.9074 - fmeasure: 0.9496 - val_loss: 0.0856 - val_acc: 0.9743 - val_precision: 0.9469 - val_recall: 0.5155 - val_fmeasure: 0.6635\n",
      "Epoch 146/500\n",
      "7s - loss: 0.0340 - acc: 0.9951 - precision: 0.9965 - recall: 0.9058 - fmeasure: 0.9483 - val_loss: 0.0857 - val_acc: 0.9742 - val_precision: 0.9490 - val_recall: 0.5131 - val_fmeasure: 0.6619\n",
      "Epoch 147/500\n",
      "7s - loss: 0.0337 - acc: 0.9953 - precision: 0.9969 - recall: 0.9082 - fmeasure: 0.9497 - val_loss: 0.0854 - val_acc: 0.9743 - val_precision: 0.9472 - val_recall: 0.5150 - val_fmeasure: 0.6631\n",
      "Epoch 148/500\n",
      "7s - loss: 0.0337 - acc: 0.9952 - precision: 0.9970 - recall: 0.9063 - fmeasure: 0.9487 - val_loss: 0.0855 - val_acc: 0.9744 - val_precision: 0.9481 - val_recall: 0.5161 - val_fmeasure: 0.6642\n",
      "Epoch 149/500\n",
      "7s - loss: 0.0333 - acc: 0.9954 - precision: 0.9974 - recall: 0.9105 - fmeasure: 0.9511 - val_loss: 0.0856 - val_acc: 0.9743 - val_precision: 0.9488 - val_recall: 0.5141 - val_fmeasure: 0.6626\n",
      "Epoch 150/500\n",
      "7s - loss: 0.0332 - acc: 0.9954 - precision: 0.9973 - recall: 0.9101 - fmeasure: 0.9508 - val_loss: 0.0851 - val_acc: 0.9745 - val_precision: 0.9484 - val_recall: 0.5183 - val_fmeasure: 0.6659\n",
      "Epoch 151/500\n",
      "7s - loss: 0.0330 - acc: 0.9953 - precision: 0.9971 - recall: 0.9079 - fmeasure: 0.9496 - val_loss: 0.0853 - val_acc: 0.9743 - val_precision: 0.9501 - val_recall: 0.5143 - val_fmeasure: 0.6632\n",
      "Epoch 152/500\n",
      "7s - loss: 0.0328 - acc: 0.9955 - precision: 0.9970 - recall: 0.9130 - fmeasure: 0.9525 - val_loss: 0.0850 - val_acc: 0.9746 - val_precision: 0.9491 - val_recall: 0.5200 - val_fmeasure: 0.6678\n",
      "Epoch 153/500\n",
      "7s - loss: 0.0327 - acc: 0.9954 - precision: 0.9967 - recall: 0.9118 - fmeasure: 0.9516 - val_loss: 0.0849 - val_acc: 0.9746 - val_precision: 0.9494 - val_recall: 0.5199 - val_fmeasure: 0.6677\n",
      "Epoch 154/500\n",
      "7s - loss: 0.0324 - acc: 0.9956 - precision: 0.9965 - recall: 0.9151 - fmeasure: 0.9533 - val_loss: 0.0850 - val_acc: 0.9746 - val_precision: 0.9491 - val_recall: 0.5198 - val_fmeasure: 0.6675\n",
      "Epoch 155/500\n",
      "7s - loss: 0.0323 - acc: 0.9955 - precision: 0.9970 - recall: 0.9132 - fmeasure: 0.9526 - val_loss: 0.0849 - val_acc: 0.9746 - val_precision: 0.9495 - val_recall: 0.5200 - val_fmeasure: 0.6679\n",
      "Epoch 156/500\n",
      "7s - loss: 0.0321 - acc: 0.9957 - precision: 0.9972 - recall: 0.9173 - fmeasure: 0.9548 - val_loss: 0.0848 - val_acc: 0.9746 - val_precision: 0.9483 - val_recall: 0.5207 - val_fmeasure: 0.6680\n",
      "Epoch 157/500\n",
      "7s - loss: 0.0320 - acc: 0.9955 - precision: 0.9974 - recall: 0.9128 - fmeasure: 0.9525 - val_loss: 0.0848 - val_acc: 0.9745 - val_precision: 0.9483 - val_recall: 0.5193 - val_fmeasure: 0.6670\n",
      "Epoch 158/500\n",
      "7s - loss: 0.0319 - acc: 0.9956 - precision: 0.9972 - recall: 0.9147 - fmeasure: 0.9535 - val_loss: 0.0844 - val_acc: 0.9748 - val_precision: 0.9486 - val_recall: 0.5240 - val_fmeasure: 0.6710\n",
      "Epoch 159/500\n",
      "7s - loss: 0.0318 - acc: 0.9956 - precision: 0.9965 - recall: 0.9158 - fmeasure: 0.9536 - val_loss: 0.0843 - val_acc: 0.9748 - val_precision: 0.9486 - val_recall: 0.5247 - val_fmeasure: 0.6716\n",
      "Epoch 160/500\n",
      "7s - loss: 0.0317 - acc: 0.9957 - precision: 0.9967 - recall: 0.9163 - fmeasure: 0.9541 - val_loss: 0.0845 - val_acc: 0.9748 - val_precision: 0.9491 - val_recall: 0.5247 - val_fmeasure: 0.6717\n",
      "Epoch 161/500\n",
      "7s - loss: 0.0315 - acc: 0.9957 - precision: 0.9972 - recall: 0.9175 - fmeasure: 0.9550 - val_loss: 0.0843 - val_acc: 0.9747 - val_precision: 0.9484 - val_recall: 0.5238 - val_fmeasure: 0.6707\n",
      "Epoch 162/500\n",
      "7s - loss: 0.0312 - acc: 0.9958 - precision: 0.9972 - recall: 0.9193 - fmeasure: 0.9560 - val_loss: 0.0843 - val_acc: 0.9748 - val_precision: 0.9479 - val_recall: 0.5246 - val_fmeasure: 0.6712\n",
      "Epoch 163/500\n",
      "7s - loss: 0.0312 - acc: 0.9958 - precision: 0.9966 - recall: 0.9185 - fmeasure: 0.9553 - val_loss: 0.0843 - val_acc: 0.9748 - val_precision: 0.9503 - val_recall: 0.5230 - val_fmeasure: 0.6706\n",
      "Epoch 164/500\n",
      "7s - loss: 0.0310 - acc: 0.9958 - precision: 0.9968 - recall: 0.9197 - fmeasure: 0.9559 - val_loss: 0.0839 - val_acc: 0.9749 - val_precision: 0.9488 - val_recall: 0.5260 - val_fmeasure: 0.6728\n",
      "Epoch 165/500\n",
      "7s - loss: 0.0308 - acc: 0.9959 - precision: 0.9973 - recall: 0.9197 - fmeasure: 0.9563 - val_loss: 0.0839 - val_acc: 0.9749 - val_precision: 0.9491 - val_recall: 0.5263 - val_fmeasure: 0.6729\n",
      "Epoch 166/500\n",
      "7s - loss: 0.0308 - acc: 0.9957 - precision: 0.9967 - recall: 0.9162 - fmeasure: 0.9541 - val_loss: 0.0839 - val_acc: 0.9748 - val_precision: 0.9483 - val_recall: 0.5262 - val_fmeasure: 0.6726\n",
      "Epoch 167/500\n",
      "7s - loss: 0.0304 - acc: 0.9959 - precision: 0.9975 - recall: 0.9212 - fmeasure: 0.9572 - val_loss: 0.0838 - val_acc: 0.9749 - val_precision: 0.9486 - val_recall: 0.5275 - val_fmeasure: 0.6737\n",
      "Epoch 168/500\n",
      "7s - loss: 0.0304 - acc: 0.9960 - precision: 0.9973 - recall: 0.9228 - fmeasure: 0.9581 - val_loss: 0.0838 - val_acc: 0.9749 - val_precision: 0.9490 - val_recall: 0.5268 - val_fmeasure: 0.6734\n",
      "Epoch 169/500\n",
      "7s - loss: 0.0304 - acc: 0.9960 - precision: 0.9976 - recall: 0.9224 - fmeasure: 0.9579 - val_loss: 0.0837 - val_acc: 0.9751 - val_precision: 0.9479 - val_recall: 0.5313 - val_fmeasure: 0.6768\n",
      "Epoch 170/500\n",
      "7s - loss: 0.0301 - acc: 0.9961 - precision: 0.9976 - recall: 0.9234 - fmeasure: 0.9584 - val_loss: 0.0837 - val_acc: 0.9750 - val_precision: 0.9480 - val_recall: 0.5291 - val_fmeasure: 0.6749\n",
      "Epoch 171/500\n",
      "7s - loss: 0.0300 - acc: 0.9961 - precision: 0.9979 - recall: 0.9240 - fmeasure: 0.9589 - val_loss: 0.0835 - val_acc: 0.9750 - val_precision: 0.9484 - val_recall: 0.5288 - val_fmeasure: 0.6750\n",
      "Epoch 172/500\n",
      "7s - loss: 0.0298 - acc: 0.9962 - precision: 0.9978 - recall: 0.9266 - fmeasure: 0.9602 - val_loss: 0.0833 - val_acc: 0.9751 - val_precision: 0.9502 - val_recall: 0.5312 - val_fmeasure: 0.6772\n",
      "Epoch 173/500\n",
      "7s - loss: 0.0297 - acc: 0.9962 - precision: 0.9978 - recall: 0.9258 - fmeasure: 0.9598 - val_loss: 0.0834 - val_acc: 0.9750 - val_precision: 0.9478 - val_recall: 0.5301 - val_fmeasure: 0.6758\n",
      "Epoch 174/500\n",
      "7s - loss: 0.0296 - acc: 0.9960 - precision: 0.9969 - recall: 0.9238 - fmeasure: 0.9584 - val_loss: 0.0833 - val_acc: 0.9751 - val_precision: 0.9500 - val_recall: 0.5300 - val_fmeasure: 0.6762\n",
      "Epoch 175/500\n",
      "7s - loss: 0.0295 - acc: 0.9961 - precision: 0.9970 - recall: 0.9254 - fmeasure: 0.9592 - val_loss: 0.0832 - val_acc: 0.9750 - val_precision: 0.9492 - val_recall: 0.5291 - val_fmeasure: 0.6753\n",
      "Epoch 176/500\n",
      "7s - loss: 0.0294 - acc: 0.9962 - precision: 0.9977 - recall: 0.9266 - fmeasure: 0.9602 - val_loss: 0.0832 - val_acc: 0.9751 - val_precision: 0.9478 - val_recall: 0.5323 - val_fmeasure: 0.6776\n",
      "Epoch 177/500\n",
      "7s - loss: 0.0294 - acc: 0.9963 - precision: 0.9980 - recall: 0.9270 - fmeasure: 0.9605 - val_loss: 0.0831 - val_acc: 0.9752 - val_precision: 0.9468 - val_recall: 0.5357 - val_fmeasure: 0.6802\n",
      "Epoch 178/500\n",
      "7s - loss: 0.0289 - acc: 0.9963 - precision: 0.9976 - recall: 0.9289 - fmeasure: 0.9615 - val_loss: 0.0832 - val_acc: 0.9751 - val_precision: 0.9491 - val_recall: 0.5309 - val_fmeasure: 0.6769\n",
      "Epoch 179/500\n",
      "7s - loss: 0.0289 - acc: 0.9963 - precision: 0.9968 - recall: 0.9296 - fmeasure: 0.9615 - val_loss: 0.0830 - val_acc: 0.9752 - val_precision: 0.9495 - val_recall: 0.5323 - val_fmeasure: 0.6781\n",
      "Epoch 180/500\n",
      "7s - loss: 0.0288 - acc: 0.9964 - precision: 0.9976 - recall: 0.9306 - fmeasure: 0.9624 - val_loss: 0.0828 - val_acc: 0.9752 - val_precision: 0.9474 - val_recall: 0.5340 - val_fmeasure: 0.6789\n",
      "Epoch 181/500\n",
      "7s - loss: 0.0287 - acc: 0.9963 - precision: 0.9980 - recall: 0.9278 - fmeasure: 0.9611 - val_loss: 0.0828 - val_acc: 0.9752 - val_precision: 0.9486 - val_recall: 0.5335 - val_fmeasure: 0.6788\n",
      "Epoch 182/500\n",
      "7s - loss: 0.0287 - acc: 0.9964 - precision: 0.9971 - recall: 0.9308 - fmeasure: 0.9622 - val_loss: 0.0829 - val_acc: 0.9752 - val_precision: 0.9482 - val_recall: 0.5341 - val_fmeasure: 0.6793\n",
      "Epoch 183/500\n",
      "7s - loss: 0.0283 - acc: 0.9965 - precision: 0.9978 - recall: 0.9315 - fmeasure: 0.9630 - val_loss: 0.0827 - val_acc: 0.9753 - val_precision: 0.9480 - val_recall: 0.5353 - val_fmeasure: 0.6802\n",
      "Epoch 184/500\n",
      "7s - loss: 0.0283 - acc: 0.9966 - precision: 0.9980 - recall: 0.9333 - fmeasure: 0.9639 - val_loss: 0.0826 - val_acc: 0.9753 - val_precision: 0.9486 - val_recall: 0.5353 - val_fmeasure: 0.6803\n",
      "Epoch 185/500\n",
      "7s - loss: 0.0282 - acc: 0.9966 - precision: 0.9971 - recall: 0.9342 - fmeasure: 0.9641 - val_loss: 0.0826 - val_acc: 0.9752 - val_precision: 0.9465 - val_recall: 0.5348 - val_fmeasure: 0.6794\n",
      "Epoch 186/500\n",
      "7s - loss: 0.0281 - acc: 0.9966 - precision: 0.9979 - recall: 0.9340 - fmeasure: 0.9644 - val_loss: 0.0824 - val_acc: 0.9753 - val_precision: 0.9487 - val_recall: 0.5366 - val_fmeasure: 0.6814\n",
      "Epoch 187/500\n",
      "7s - loss: 0.0280 - acc: 0.9965 - precision: 0.9979 - recall: 0.9318 - fmeasure: 0.9631 - val_loss: 0.0829 - val_acc: 0.9753 - val_precision: 0.9492 - val_recall: 0.5345 - val_fmeasure: 0.6798\n",
      "Epoch 188/500\n",
      "7s - loss: 0.0277 - acc: 0.9966 - precision: 0.9976 - recall: 0.9340 - fmeasure: 0.9642 - val_loss: 0.0827 - val_acc: 0.9753 - val_precision: 0.9495 - val_recall: 0.5349 - val_fmeasure: 0.6802\n",
      "Epoch 189/500\n",
      "7s - loss: 0.0277 - acc: 0.9966 - precision: 0.9976 - recall: 0.9341 - fmeasure: 0.9642 - val_loss: 0.0824 - val_acc: 0.9755 - val_precision: 0.9480 - val_recall: 0.5397 - val_fmeasure: 0.6838\n",
      "Epoch 190/500\n",
      "7s - loss: 0.0275 - acc: 0.9966 - precision: 0.9971 - recall: 0.9346 - fmeasure: 0.9643 - val_loss: 0.0824 - val_acc: 0.9754 - val_precision: 0.9504 - val_recall: 0.5361 - val_fmeasure: 0.6815\n",
      "Epoch 191/500\n",
      "7s - loss: 0.0276 - acc: 0.9965 - precision: 0.9971 - recall: 0.9325 - fmeasure: 0.9630 - val_loss: 0.0821 - val_acc: 0.9755 - val_precision: 0.9495 - val_recall: 0.5389 - val_fmeasure: 0.6835\n",
      "Epoch 192/500\n",
      "7s - loss: 0.0275 - acc: 0.9966 - precision: 0.9982 - recall: 0.9342 - fmeasure: 0.9646 - val_loss: 0.0820 - val_acc: 0.9754 - val_precision: 0.9471 - val_recall: 0.5393 - val_fmeasure: 0.6832\n",
      "Epoch 193/500\n",
      "7s - loss: 0.0273 - acc: 0.9967 - precision: 0.9977 - recall: 0.9363 - fmeasure: 0.9655 - val_loss: 0.0824 - val_acc: 0.9753 - val_precision: 0.9483 - val_recall: 0.5360 - val_fmeasure: 0.6808\n",
      "Epoch 194/500\n",
      "7s - loss: 0.0273 - acc: 0.9965 - precision: 0.9973 - recall: 0.9335 - fmeasure: 0.9637 - val_loss: 0.0822 - val_acc: 0.9754 - val_precision: 0.9488 - val_recall: 0.5376 - val_fmeasure: 0.6822\n",
      "Epoch 195/500\n",
      "7s - loss: 0.0271 - acc: 0.9966 - precision: 0.9968 - recall: 0.9359 - fmeasure: 0.9649 - val_loss: 0.0821 - val_acc: 0.9755 - val_precision: 0.9504 - val_recall: 0.5390 - val_fmeasure: 0.6839\n",
      "Epoch 196/500\n",
      "7s - loss: 0.0271 - acc: 0.9967 - precision: 0.9970 - recall: 0.9372 - fmeasure: 0.9656 - val_loss: 0.0819 - val_acc: 0.9755 - val_precision: 0.9483 - val_recall: 0.5400 - val_fmeasure: 0.6841\n",
      "Epoch 197/500\n",
      "7s - loss: 0.0269 - acc: 0.9968 - precision: 0.9980 - recall: 0.9382 - fmeasure: 0.9667 - val_loss: 0.0819 - val_acc: 0.9755 - val_precision: 0.9481 - val_recall: 0.5408 - val_fmeasure: 0.6848\n",
      "Epoch 198/500\n",
      "7s - loss: 0.0267 - acc: 0.9967 - precision: 0.9978 - recall: 0.9369 - fmeasure: 0.9658 - val_loss: 0.0819 - val_acc: 0.9755 - val_precision: 0.9477 - val_recall: 0.5401 - val_fmeasure: 0.6841\n",
      "Epoch 199/500\n",
      "7s - loss: 0.0268 - acc: 0.9968 - precision: 0.9977 - recall: 0.9390 - fmeasure: 0.9670 - val_loss: 0.0820 - val_acc: 0.9755 - val_precision: 0.9494 - val_recall: 0.5397 - val_fmeasure: 0.6842\n",
      "Epoch 200/500\n",
      "7s - loss: 0.0265 - acc: 0.9969 - precision: 0.9983 - recall: 0.9391 - fmeasure: 0.9673 - val_loss: 0.0820 - val_acc: 0.9754 - val_precision: 0.9479 - val_recall: 0.5392 - val_fmeasure: 0.6834\n",
      "Epoch 201/500\n",
      "7s - loss: 0.0266 - acc: 0.9967 - precision: 0.9972 - recall: 0.9371 - fmeasure: 0.9657 - val_loss: 0.0814 - val_acc: 0.9757 - val_precision: 0.9488 - val_recall: 0.5439 - val_fmeasure: 0.6875\n",
      "Epoch 202/500\n",
      "7s - loss: 0.0264 - acc: 0.9968 - precision: 0.9978 - recall: 0.9383 - fmeasure: 0.9667 - val_loss: 0.0814 - val_acc: 0.9756 - val_precision: 0.9468 - val_recall: 0.5430 - val_fmeasure: 0.6862\n",
      "Epoch 203/500\n",
      "7s - loss: 0.0262 - acc: 0.9968 - precision: 0.9975 - recall: 0.9380 - fmeasure: 0.9664 - val_loss: 0.0819 - val_acc: 0.9755 - val_precision: 0.9477 - val_recall: 0.5396 - val_fmeasure: 0.6836\n",
      "Epoch 204/500\n",
      "7s - loss: 0.0262 - acc: 0.9969 - precision: 0.9977 - recall: 0.9408 - fmeasure: 0.9679 - val_loss: 0.0816 - val_acc: 0.9756 - val_precision: 0.9485 - val_recall: 0.5416 - val_fmeasure: 0.6855\n",
      "Epoch 205/500\n",
      "7s - loss: 0.0260 - acc: 0.9971 - precision: 0.9980 - recall: 0.9445 - fmeasure: 0.9700 - val_loss: 0.0817 - val_acc: 0.9754 - val_precision: 0.9472 - val_recall: 0.5393 - val_fmeasure: 0.6833\n",
      "Epoch 206/500\n",
      "7s - loss: 0.0260 - acc: 0.9969 - precision: 0.9981 - recall: 0.9397 - fmeasure: 0.9675 - val_loss: 0.0816 - val_acc: 0.9754 - val_precision: 0.9469 - val_recall: 0.5398 - val_fmeasure: 0.6837\n",
      "Epoch 207/500\n",
      "7s - loss: 0.0260 - acc: 0.9970 - precision: 0.9978 - recall: 0.9415 - fmeasure: 0.9683 - val_loss: 0.0812 - val_acc: 0.9757 - val_precision: 0.9482 - val_recall: 0.5450 - val_fmeasure: 0.6882\n",
      "Epoch 208/500\n",
      "7s - loss: 0.0258 - acc: 0.9971 - precision: 0.9977 - recall: 0.9433 - fmeasure: 0.9693 - val_loss: 0.0813 - val_acc: 0.9756 - val_precision: 0.9467 - val_recall: 0.5443 - val_fmeasure: 0.6874\n",
      "Epoch 209/500\n",
      "7s - loss: 0.0257 - acc: 0.9971 - precision: 0.9974 - recall: 0.9440 - fmeasure: 0.9695 - val_loss: 0.0813 - val_acc: 0.9756 - val_precision: 0.9489 - val_recall: 0.5429 - val_fmeasure: 0.6866\n",
      "Epoch 210/500\n",
      "7s - loss: 0.0257 - acc: 0.9970 - precision: 0.9979 - recall: 0.9415 - fmeasure: 0.9684 - val_loss: 0.0811 - val_acc: 0.9757 - val_precision: 0.9488 - val_recall: 0.5433 - val_fmeasure: 0.6869\n",
      "Epoch 211/500\n",
      "7s - loss: 0.0255 - acc: 0.9970 - precision: 0.9983 - recall: 0.9413 - fmeasure: 0.9685 - val_loss: 0.0811 - val_acc: 0.9758 - val_precision: 0.9478 - val_recall: 0.5462 - val_fmeasure: 0.6890\n",
      "Epoch 212/500\n",
      "7s - loss: 0.0254 - acc: 0.9970 - precision: 0.9977 - recall: 0.9430 - fmeasure: 0.9691 - val_loss: 0.0810 - val_acc: 0.9757 - val_precision: 0.9471 - val_recall: 0.5462 - val_fmeasure: 0.6890\n",
      "Epoch 213/500\n",
      "7s - loss: 0.0253 - acc: 0.9971 - precision: 0.9981 - recall: 0.9435 - fmeasure: 0.9696 - val_loss: 0.0810 - val_acc: 0.9758 - val_precision: 0.9485 - val_recall: 0.5459 - val_fmeasure: 0.6890\n",
      "Epoch 214/500\n",
      "7s - loss: 0.0252 - acc: 0.9971 - precision: 0.9983 - recall: 0.9431 - fmeasure: 0.9694 - val_loss: 0.0812 - val_acc: 0.9758 - val_precision: 0.9485 - val_recall: 0.5459 - val_fmeasure: 0.6892\n",
      "Epoch 215/500\n",
      "7s - loss: 0.0251 - acc: 0.9972 - precision: 0.9982 - recall: 0.9464 - fmeasure: 0.9711 - val_loss: 0.0809 - val_acc: 0.9759 - val_precision: 0.9501 - val_recall: 0.5469 - val_fmeasure: 0.6902\n",
      "Epoch 216/500\n",
      "7s - loss: 0.0250 - acc: 0.9972 - precision: 0.9978 - recall: 0.9453 - fmeasure: 0.9703 - val_loss: 0.0810 - val_acc: 0.9757 - val_precision: 0.9473 - val_recall: 0.5457 - val_fmeasure: 0.6886\n",
      "Epoch 217/500\n",
      "7s - loss: 0.0249 - acc: 0.9972 - precision: 0.9977 - recall: 0.9460 - fmeasure: 0.9707 - val_loss: 0.0808 - val_acc: 0.9758 - val_precision: 0.9464 - val_recall: 0.5474 - val_fmeasure: 0.6899\n",
      "Epoch 218/500\n",
      "7s - loss: 0.0250 - acc: 0.9971 - precision: 0.9978 - recall: 0.9433 - fmeasure: 0.9694 - val_loss: 0.0807 - val_acc: 0.9759 - val_precision: 0.9470 - val_recall: 0.5489 - val_fmeasure: 0.6911\n",
      "Epoch 219/500\n",
      "7s - loss: 0.0247 - acc: 0.9973 - precision: 0.9986 - recall: 0.9475 - fmeasure: 0.9720 - val_loss: 0.0809 - val_acc: 0.9758 - val_precision: 0.9489 - val_recall: 0.5463 - val_fmeasure: 0.6896\n",
      "Epoch 220/500\n",
      "7s - loss: 0.0247 - acc: 0.9972 - precision: 0.9977 - recall: 0.9456 - fmeasure: 0.9705 - val_loss: 0.0808 - val_acc: 0.9758 - val_precision: 0.9485 - val_recall: 0.5462 - val_fmeasure: 0.6894\n",
      "Epoch 221/500\n",
      "7s - loss: 0.0248 - acc: 0.9973 - precision: 0.9984 - recall: 0.9468 - fmeasure: 0.9715 - val_loss: 0.0807 - val_acc: 0.9759 - val_precision: 0.9468 - val_recall: 0.5498 - val_fmeasure: 0.6919\n",
      "Epoch 222/500\n",
      "7s - loss: 0.0246 - acc: 0.9973 - precision: 0.9984 - recall: 0.9474 - fmeasure: 0.9718 - val_loss: 0.0805 - val_acc: 0.9759 - val_precision: 0.9463 - val_recall: 0.5503 - val_fmeasure: 0.6921\n",
      "Epoch 223/500\n",
      "7s - loss: 0.0244 - acc: 0.9973 - precision: 0.9983 - recall: 0.9477 - fmeasure: 0.9719 - val_loss: 0.0809 - val_acc: 0.9759 - val_precision: 0.9490 - val_recall: 0.5481 - val_fmeasure: 0.6910\n",
      "Epoch 224/500\n",
      "7s - loss: 0.0245 - acc: 0.9971 - precision: 0.9968 - recall: 0.9459 - fmeasure: 0.9703 - val_loss: 0.0807 - val_acc: 0.9758 - val_precision: 0.9470 - val_recall: 0.5477 - val_fmeasure: 0.6901\n",
      "Epoch 225/500\n",
      "7s - loss: 0.0244 - acc: 0.9973 - precision: 0.9985 - recall: 0.9466 - fmeasure: 0.9715 - val_loss: 0.0802 - val_acc: 0.9760 - val_precision: 0.9471 - val_recall: 0.5511 - val_fmeasure: 0.6929\n",
      "Epoch 226/500\n",
      "7s - loss: 0.0242 - acc: 0.9973 - precision: 0.9976 - recall: 0.9480 - fmeasure: 0.9718 - val_loss: 0.0807 - val_acc: 0.9758 - val_precision: 0.9465 - val_recall: 0.5481 - val_fmeasure: 0.6903\n",
      "Epoch 227/500\n",
      "7s - loss: 0.0241 - acc: 0.9975 - precision: 0.9978 - recall: 0.9511 - fmeasure: 0.9735 - val_loss: 0.0804 - val_acc: 0.9759 - val_precision: 0.9471 - val_recall: 0.5493 - val_fmeasure: 0.6914\n",
      "Epoch 228/500\n",
      "7s - loss: 0.0240 - acc: 0.9974 - precision: 0.9980 - recall: 0.9494 - fmeasure: 0.9727 - val_loss: 0.0805 - val_acc: 0.9758 - val_precision: 0.9458 - val_recall: 0.5489 - val_fmeasure: 0.6907\n",
      "Epoch 229/500\n",
      "7s - loss: 0.0239 - acc: 0.9973 - precision: 0.9974 - recall: 0.9494 - fmeasure: 0.9724 - val_loss: 0.0804 - val_acc: 0.9758 - val_precision: 0.9483 - val_recall: 0.5477 - val_fmeasure: 0.6905\n",
      "Epoch 230/500\n",
      "7s - loss: 0.0239 - acc: 0.9974 - precision: 0.9983 - recall: 0.9495 - fmeasure: 0.9729 - val_loss: 0.0803 - val_acc: 0.9759 - val_precision: 0.9488 - val_recall: 0.5487 - val_fmeasure: 0.6915\n",
      "Epoch 231/500\n",
      "7s - loss: 0.0238 - acc: 0.9974 - precision: 0.9984 - recall: 0.9497 - fmeasure: 0.9731 - val_loss: 0.0803 - val_acc: 0.9759 - val_precision: 0.9479 - val_recall: 0.5497 - val_fmeasure: 0.6919\n",
      "Epoch 232/500\n",
      "7s - loss: 0.0236 - acc: 0.9975 - precision: 0.9984 - recall: 0.9520 - fmeasure: 0.9743 - val_loss: 0.0805 - val_acc: 0.9758 - val_precision: 0.9470 - val_recall: 0.5483 - val_fmeasure: 0.6907\n",
      "Epoch 233/500\n",
      "7s - loss: 0.0236 - acc: 0.9975 - precision: 0.9981 - recall: 0.9516 - fmeasure: 0.9739 - val_loss: 0.0805 - val_acc: 0.9759 - val_precision: 0.9491 - val_recall: 0.5475 - val_fmeasure: 0.6906\n",
      "Epoch 234/500\n",
      "7s - loss: 0.0236 - acc: 0.9974 - precision: 0.9985 - recall: 0.9499 - fmeasure: 0.9731 - val_loss: 0.0802 - val_acc: 0.9760 - val_precision: 0.9477 - val_recall: 0.5508 - val_fmeasure: 0.6929\n",
      "Epoch 235/500\n",
      "7s - loss: 0.0236 - acc: 0.9974 - precision: 0.9977 - recall: 0.9494 - fmeasure: 0.9725 - val_loss: 0.0802 - val_acc: 0.9760 - val_precision: 0.9481 - val_recall: 0.5519 - val_fmeasure: 0.6938\n",
      "Epoch 236/500\n",
      "7s - loss: 0.0233 - acc: 0.9976 - precision: 0.9985 - recall: 0.9528 - fmeasure: 0.9747 - val_loss: 0.0801 - val_acc: 0.9758 - val_precision: 0.9470 - val_recall: 0.5479 - val_fmeasure: 0.6904\n",
      "Epoch 237/500\n",
      "7s - loss: 0.0233 - acc: 0.9975 - precision: 0.9980 - recall: 0.9516 - fmeasure: 0.9739 - val_loss: 0.0798 - val_acc: 0.9760 - val_precision: 0.9476 - val_recall: 0.5515 - val_fmeasure: 0.6933\n",
      "Epoch 238/500\n",
      "7s - loss: 0.0232 - acc: 0.9976 - precision: 0.9982 - recall: 0.9533 - fmeasure: 0.9749 - val_loss: 0.0802 - val_acc: 0.9760 - val_precision: 0.9468 - val_recall: 0.5523 - val_fmeasure: 0.6937\n",
      "Epoch 239/500\n",
      "7s - loss: 0.0233 - acc: 0.9975 - precision: 0.9977 - recall: 0.9517 - fmeasure: 0.9738 - val_loss: 0.0800 - val_acc: 0.9760 - val_precision: 0.9476 - val_recall: 0.5522 - val_fmeasure: 0.6939\n",
      "Epoch 240/500\n",
      "7s - loss: 0.0231 - acc: 0.9975 - precision: 0.9980 - recall: 0.9516 - fmeasure: 0.9738 - val_loss: 0.0803 - val_acc: 0.9759 - val_precision: 0.9493 - val_recall: 0.5478 - val_fmeasure: 0.6908\n",
      "Epoch 241/500\n",
      "7s - loss: 0.0229 - acc: 0.9976 - precision: 0.9981 - recall: 0.9536 - fmeasure: 0.9750 - val_loss: 0.0801 - val_acc: 0.9760 - val_precision: 0.9496 - val_recall: 0.5498 - val_fmeasure: 0.6926\n",
      "Epoch 242/500\n",
      "7s - loss: 0.0229 - acc: 0.9975 - precision: 0.9985 - recall: 0.9524 - fmeasure: 0.9745 - val_loss: 0.0801 - val_acc: 0.9760 - val_precision: 0.9470 - val_recall: 0.5508 - val_fmeasure: 0.6927\n",
      "Epoch 243/500\n",
      "7s - loss: 0.0228 - acc: 0.9976 - precision: 0.9983 - recall: 0.9534 - fmeasure: 0.9750 - val_loss: 0.0802 - val_acc: 0.9759 - val_precision: 0.9483 - val_recall: 0.5486 - val_fmeasure: 0.6913\n",
      "Epoch 244/500\n",
      "7s - loss: 0.0228 - acc: 0.9976 - precision: 0.9977 - recall: 0.9549 - fmeasure: 0.9755 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9468 - val_recall: 0.5535 - val_fmeasure: 0.6946\n",
      "Epoch 245/500\n",
      "7s - loss: 0.0228 - acc: 0.9976 - precision: 0.9981 - recall: 0.9548 - fmeasure: 0.9756 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9475 - val_recall: 0.5538 - val_fmeasure: 0.6951\n",
      "Epoch 246/500\n",
      "7s - loss: 0.0227 - acc: 0.9975 - precision: 0.9978 - recall: 0.9519 - fmeasure: 0.9739 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9472 - val_recall: 0.5543 - val_fmeasure: 0.6955\n",
      "Epoch 247/500\n",
      "7s - loss: 0.0225 - acc: 0.9977 - precision: 0.9978 - recall: 0.9555 - fmeasure: 0.9759 - val_loss: 0.0800 - val_acc: 0.9760 - val_precision: 0.9481 - val_recall: 0.5510 - val_fmeasure: 0.6931\n",
      "Epoch 248/500\n",
      "7s - loss: 0.0226 - acc: 0.9976 - precision: 0.9982 - recall: 0.9543 - fmeasure: 0.9754 - val_loss: 0.0796 - val_acc: 0.9761 - val_precision: 0.9449 - val_recall: 0.5546 - val_fmeasure: 0.6950\n",
      "Epoch 249/500\n",
      "7s - loss: 0.0225 - acc: 0.9977 - precision: 0.9991 - recall: 0.9542 - fmeasure: 0.9758 - val_loss: 0.0798 - val_acc: 0.9761 - val_precision: 0.9472 - val_recall: 0.5535 - val_fmeasure: 0.6948\n",
      "Epoch 250/500\n",
      "7s - loss: 0.0224 - acc: 0.9977 - precision: 0.9983 - recall: 0.9555 - fmeasure: 0.9760 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9483 - val_recall: 0.5526 - val_fmeasure: 0.6944\n",
      "Epoch 251/500\n",
      "7s - loss: 0.0224 - acc: 0.9977 - precision: 0.9985 - recall: 0.9552 - fmeasure: 0.9760 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9480 - val_recall: 0.5535 - val_fmeasure: 0.6951\n",
      "Epoch 252/500\n",
      "7s - loss: 0.0222 - acc: 0.9978 - precision: 0.9988 - recall: 0.9566 - fmeasure: 0.9769 - val_loss: 0.0794 - val_acc: 0.9762 - val_precision: 0.9475 - val_recall: 0.5552 - val_fmeasure: 0.6964\n",
      "Epoch 253/500\n",
      "7s - loss: 0.0222 - acc: 0.9977 - precision: 0.9982 - recall: 0.9553 - fmeasure: 0.9759 - val_loss: 0.0798 - val_acc: 0.9761 - val_precision: 0.9467 - val_recall: 0.5540 - val_fmeasure: 0.6951\n",
      "Epoch 254/500\n",
      "7s - loss: 0.0221 - acc: 0.9977 - precision: 0.9984 - recall: 0.9562 - fmeasure: 0.9766 - val_loss: 0.0794 - val_acc: 0.9763 - val_precision: 0.9484 - val_recall: 0.5568 - val_fmeasure: 0.6978\n",
      "Epoch 255/500\n",
      "7s - loss: 0.0219 - acc: 0.9978 - precision: 0.9985 - recall: 0.9580 - fmeasure: 0.9775 - val_loss: 0.0799 - val_acc: 0.9762 - val_precision: 0.9474 - val_recall: 0.5551 - val_fmeasure: 0.6962\n",
      "Epoch 256/500\n",
      "7s - loss: 0.0220 - acc: 0.9977 - precision: 0.9977 - recall: 0.9555 - fmeasure: 0.9757 - val_loss: 0.0795 - val_acc: 0.9761 - val_precision: 0.9483 - val_recall: 0.5532 - val_fmeasure: 0.6951\n",
      "Epoch 257/500\n",
      "7s - loss: 0.0219 - acc: 0.9977 - precision: 0.9981 - recall: 0.9565 - fmeasure: 0.9766 - val_loss: 0.0796 - val_acc: 0.9762 - val_precision: 0.9472 - val_recall: 0.5551 - val_fmeasure: 0.6962\n",
      "Epoch 258/500\n",
      "7s - loss: 0.0219 - acc: 0.9978 - precision: 0.9982 - recall: 0.9584 - fmeasure: 0.9775 - val_loss: 0.0799 - val_acc: 0.9761 - val_precision: 0.9479 - val_recall: 0.5523 - val_fmeasure: 0.6940\n",
      "Epoch 259/500\n",
      "7s - loss: 0.0217 - acc: 0.9978 - precision: 0.9983 - recall: 0.9578 - fmeasure: 0.9772 - val_loss: 0.0799 - val_acc: 0.9761 - val_precision: 0.9496 - val_recall: 0.5516 - val_fmeasure: 0.6942\n",
      "Epoch 260/500\n",
      "7s - loss: 0.0217 - acc: 0.9978 - precision: 0.9983 - recall: 0.9582 - fmeasure: 0.9775 - val_loss: 0.0796 - val_acc: 0.9761 - val_precision: 0.9484 - val_recall: 0.5531 - val_fmeasure: 0.6950\n",
      "Epoch 261/500\n",
      "7s - loss: 0.0216 - acc: 0.9978 - precision: 0.9985 - recall: 0.9580 - fmeasure: 0.9775 - val_loss: 0.0796 - val_acc: 0.9761 - val_precision: 0.9470 - val_recall: 0.5542 - val_fmeasure: 0.6953\n",
      "Epoch 262/500\n",
      "7s - loss: 0.0216 - acc: 0.9977 - precision: 0.9982 - recall: 0.9565 - fmeasure: 0.9765 - val_loss: 0.0796 - val_acc: 0.9762 - val_precision: 0.9466 - val_recall: 0.5562 - val_fmeasure: 0.6970\n",
      "Epoch 263/500\n",
      "7s - loss: 0.0216 - acc: 0.9977 - precision: 0.9983 - recall: 0.9566 - fmeasure: 0.9767 - val_loss: 0.0794 - val_acc: 0.9762 - val_precision: 0.9476 - val_recall: 0.5558 - val_fmeasure: 0.6969\n",
      "Epoch 264/500\n",
      "7s - loss: 0.0213 - acc: 0.9979 - precision: 0.9988 - recall: 0.9600 - fmeasure: 0.9787 - val_loss: 0.0794 - val_acc: 0.9762 - val_precision: 0.9468 - val_recall: 0.5558 - val_fmeasure: 0.6967\n",
      "Epoch 265/500\n",
      "7s - loss: 0.0214 - acc: 0.9978 - precision: 0.9983 - recall: 0.9573 - fmeasure: 0.9770 - val_loss: 0.0791 - val_acc: 0.9763 - val_precision: 0.9478 - val_recall: 0.5575 - val_fmeasure: 0.6983\n",
      "Epoch 266/500\n",
      "7s - loss: 0.0213 - acc: 0.9979 - precision: 0.9989 - recall: 0.9585 - fmeasure: 0.9779 - val_loss: 0.0793 - val_acc: 0.9762 - val_precision: 0.9479 - val_recall: 0.5554 - val_fmeasure: 0.6965\n",
      "Epoch 267/500\n",
      "7s - loss: 0.0212 - acc: 0.9979 - precision: 0.9987 - recall: 0.9596 - fmeasure: 0.9784 - val_loss: 0.0791 - val_acc: 0.9763 - val_precision: 0.9470 - val_recall: 0.5580 - val_fmeasure: 0.6984\n",
      "Epoch 268/500\n",
      "7s - loss: 0.0213 - acc: 0.9979 - precision: 0.9977 - recall: 0.9611 - fmeasure: 0.9786 - val_loss: 0.0797 - val_acc: 0.9761 - val_precision: 0.9483 - val_recall: 0.5536 - val_fmeasure: 0.6953\n",
      "Epoch 269/500\n",
      "7s - loss: 0.0212 - acc: 0.9980 - precision: 0.9988 - recall: 0.9604 - fmeasure: 0.9789 - val_loss: 0.0789 - val_acc: 0.9764 - val_precision: 0.9471 - val_recall: 0.5592 - val_fmeasure: 0.6995\n",
      "Epoch 270/500\n",
      "7s - loss: 0.0211 - acc: 0.9979 - precision: 0.9985 - recall: 0.9592 - fmeasure: 0.9781 - val_loss: 0.0788 - val_acc: 0.9763 - val_precision: 0.9451 - val_recall: 0.5604 - val_fmeasure: 0.6997\n",
      "Epoch 271/500\n",
      "7s - loss: 0.0210 - acc: 0.9980 - precision: 0.9984 - recall: 0.9616 - fmeasure: 0.9793 - val_loss: 0.0793 - val_acc: 0.9763 - val_precision: 0.9477 - val_recall: 0.5575 - val_fmeasure: 0.6982\n",
      "Epoch 272/500\n",
      "7s - loss: 0.0210 - acc: 0.9979 - precision: 0.9985 - recall: 0.9594 - fmeasure: 0.9782 - val_loss: 0.0791 - val_acc: 0.9762 - val_precision: 0.9467 - val_recall: 0.5566 - val_fmeasure: 0.6971\n",
      "Epoch 273/500\n",
      "7s - loss: 0.0209 - acc: 0.9980 - precision: 0.9988 - recall: 0.9609 - fmeasure: 0.9792 - val_loss: 0.0791 - val_acc: 0.9764 - val_precision: 0.9466 - val_recall: 0.5600 - val_fmeasure: 0.6998\n",
      "Epoch 274/500\n",
      "7s - loss: 0.0207 - acc: 0.9981 - precision: 0.9981 - recall: 0.9633 - fmeasure: 0.9801 - val_loss: 0.0794 - val_acc: 0.9762 - val_precision: 0.9485 - val_recall: 0.5555 - val_fmeasure: 0.6968\n",
      "Epoch 275/500\n",
      "7s - loss: 0.0209 - acc: 0.9980 - precision: 0.9982 - recall: 0.9612 - fmeasure: 0.9791 - val_loss: 0.0790 - val_acc: 0.9763 - val_precision: 0.9476 - val_recall: 0.5582 - val_fmeasure: 0.6987\n",
      "Epoch 276/500\n",
      "7s - loss: 0.0207 - acc: 0.9980 - precision: 0.9984 - recall: 0.9622 - fmeasure: 0.9796 - val_loss: 0.0789 - val_acc: 0.9763 - val_precision: 0.9470 - val_recall: 0.5586 - val_fmeasure: 0.6988\n",
      "Epoch 277/500\n",
      "7s - loss: 0.0206 - acc: 0.9979 - precision: 0.9983 - recall: 0.9597 - fmeasure: 0.9783 - val_loss: 0.0790 - val_acc: 0.9763 - val_precision: 0.9464 - val_recall: 0.5588 - val_fmeasure: 0.6989\n",
      "Epoch 278/500\n",
      "7s - loss: 0.0205 - acc: 0.9980 - precision: 0.9984 - recall: 0.9619 - fmeasure: 0.9795 - val_loss: 0.0790 - val_acc: 0.9764 - val_precision: 0.9474 - val_recall: 0.5597 - val_fmeasure: 0.6999\n",
      "Epoch 279/500\n",
      "7s - loss: 0.0205 - acc: 0.9980 - precision: 0.9986 - recall: 0.9615 - fmeasure: 0.9793 - val_loss: 0.0791 - val_acc: 0.9763 - val_precision: 0.9471 - val_recall: 0.5579 - val_fmeasure: 0.6983\n",
      "Epoch 280/500\n",
      "7s - loss: 0.0207 - acc: 0.9980 - precision: 0.9983 - recall: 0.9612 - fmeasure: 0.9791 - val_loss: 0.0791 - val_acc: 0.9763 - val_precision: 0.9473 - val_recall: 0.5587 - val_fmeasure: 0.6990\n",
      "Epoch 281/500\n",
      "7s - loss: 0.0205 - acc: 0.9980 - precision: 0.9981 - recall: 0.9620 - fmeasure: 0.9794 - val_loss: 0.0790 - val_acc: 0.9763 - val_precision: 0.9457 - val_recall: 0.5582 - val_fmeasure: 0.6981\n",
      "Epoch 282/500\n",
      "7s - loss: 0.0205 - acc: 0.9980 - precision: 0.9991 - recall: 0.9613 - fmeasure: 0.9795 - val_loss: 0.0788 - val_acc: 0.9764 - val_precision: 0.9456 - val_recall: 0.5619 - val_fmeasure: 0.7011\n",
      "Epoch 283/500\n",
      "7s - loss: 0.0205 - acc: 0.9979 - precision: 0.9982 - recall: 0.9603 - fmeasure: 0.9786 - val_loss: 0.0789 - val_acc: 0.9764 - val_precision: 0.9470 - val_recall: 0.5596 - val_fmeasure: 0.6997\n",
      "Epoch 284/500\n",
      "7s - loss: 0.0202 - acc: 0.9980 - precision: 0.9984 - recall: 0.9617 - fmeasure: 0.9794 - val_loss: 0.0788 - val_acc: 0.9764 - val_precision: 0.9465 - val_recall: 0.5612 - val_fmeasure: 0.7008\n",
      "Epoch 285/500\n",
      "7s - loss: 0.0203 - acc: 0.9981 - precision: 0.9988 - recall: 0.9626 - fmeasure: 0.9801 - val_loss: 0.0786 - val_acc: 0.9764 - val_precision: 0.9453 - val_recall: 0.5608 - val_fmeasure: 0.7002\n",
      "Epoch 286/500\n",
      "7s - loss: 0.0203 - acc: 0.9982 - precision: 0.9988 - recall: 0.9644 - fmeasure: 0.9810 - val_loss: 0.0792 - val_acc: 0.9763 - val_precision: 0.9470 - val_recall: 0.5576 - val_fmeasure: 0.6981\n",
      "Epoch 287/500\n",
      "7s - loss: 0.0201 - acc: 0.9982 - precision: 0.9988 - recall: 0.9655 - fmeasure: 0.9817 - val_loss: 0.0791 - val_acc: 0.9764 - val_precision: 0.9483 - val_recall: 0.5587 - val_fmeasure: 0.6993\n",
      "Epoch 288/500\n",
      "7s - loss: 0.0201 - acc: 0.9981 - precision: 0.9982 - recall: 0.9635 - fmeasure: 0.9802 - val_loss: 0.0787 - val_acc: 0.9764 - val_precision: 0.9467 - val_recall: 0.5603 - val_fmeasure: 0.7002\n",
      "Epoch 289/500\n",
      "7s - loss: 0.0200 - acc: 0.9981 - precision: 0.9978 - recall: 0.9636 - fmeasure: 0.9800 - val_loss: 0.0786 - val_acc: 0.9763 - val_precision: 0.9461 - val_recall: 0.5596 - val_fmeasure: 0.6995\n",
      "Epoch 290/500\n",
      "7s - loss: 0.0200 - acc: 0.9981 - precision: 0.9987 - recall: 0.9643 - fmeasure: 0.9809 - val_loss: 0.0783 - val_acc: 0.9764 - val_precision: 0.9458 - val_recall: 0.5616 - val_fmeasure: 0.7009\n",
      "Epoch 291/500\n",
      "7s - loss: 0.0200 - acc: 0.9981 - precision: 0.9982 - recall: 0.9645 - fmeasure: 0.9808 - val_loss: 0.0784 - val_acc: 0.9765 - val_precision: 0.9456 - val_recall: 0.5643 - val_fmeasure: 0.7030\n",
      "Epoch 292/500\n",
      "7s - loss: 0.0197 - acc: 0.9982 - precision: 0.9994 - recall: 0.9641 - fmeasure: 0.9812 - val_loss: 0.0788 - val_acc: 0.9764 - val_precision: 0.9465 - val_recall: 0.5599 - val_fmeasure: 0.6998\n",
      "Epoch 293/500\n",
      "7s - loss: 0.0197 - acc: 0.9982 - precision: 0.9983 - recall: 0.9659 - fmeasure: 0.9815 - val_loss: 0.0789 - val_acc: 0.9763 - val_precision: 0.9454 - val_recall: 0.5593 - val_fmeasure: 0.6992\n",
      "Epoch 294/500\n",
      "7s - loss: 0.0197 - acc: 0.9983 - precision: 0.9983 - recall: 0.9670 - fmeasure: 0.9822 - val_loss: 0.0785 - val_acc: 0.9764 - val_precision: 0.9468 - val_recall: 0.5599 - val_fmeasure: 0.7000\n",
      "Epoch 295/500\n",
      "7s - loss: 0.0198 - acc: 0.9981 - precision: 0.9982 - recall: 0.9634 - fmeasure: 0.9802 - val_loss: 0.0787 - val_acc: 0.9764 - val_precision: 0.9461 - val_recall: 0.5611 - val_fmeasure: 0.7005\n",
      "Epoch 296/500\n",
      "7s - loss: 0.0197 - acc: 0.9982 - precision: 0.9980 - recall: 0.9653 - fmeasure: 0.9811 - val_loss: 0.0790 - val_acc: 0.9764 - val_precision: 0.9474 - val_recall: 0.5611 - val_fmeasure: 0.7010\n",
      "Epoch 297/500\n",
      "7s - loss: 0.0197 - acc: 0.9982 - precision: 0.9985 - recall: 0.9650 - fmeasure: 0.9811 - val_loss: 0.0785 - val_acc: 0.9765 - val_precision: 0.9466 - val_recall: 0.5627 - val_fmeasure: 0.7020\n",
      "Epoch 298/500\n",
      "7s - loss: 0.0194 - acc: 0.9983 - precision: 0.9984 - recall: 0.9682 - fmeasure: 0.9828 - val_loss: 0.0786 - val_acc: 0.9765 - val_precision: 0.9469 - val_recall: 0.5629 - val_fmeasure: 0.7024\n",
      "Epoch 299/500\n",
      "7s - loss: 0.0195 - acc: 0.9982 - precision: 0.9987 - recall: 0.9654 - fmeasure: 0.9815 - val_loss: 0.0785 - val_acc: 0.9765 - val_precision: 0.9449 - val_recall: 0.5631 - val_fmeasure: 0.7017\n",
      "Epoch 300/500\n",
      "7s - loss: 0.0194 - acc: 0.9982 - precision: 0.9981 - recall: 0.9653 - fmeasure: 0.9812 - val_loss: 0.0785 - val_acc: 0.9764 - val_precision: 0.9457 - val_recall: 0.5615 - val_fmeasure: 0.7008\n",
      "Epoch 301/500\n",
      "7s - loss: 0.0196 - acc: 0.9980 - precision: 0.9985 - recall: 0.9623 - fmeasure: 0.9798 - val_loss: 0.0782 - val_acc: 0.9766 - val_precision: 0.9460 - val_recall: 0.5657 - val_fmeasure: 0.7043\n",
      "Epoch 302/500\n",
      "7s - loss: 0.0194 - acc: 0.9982 - precision: 0.9979 - recall: 0.9656 - fmeasure: 0.9812 - val_loss: 0.0789 - val_acc: 0.9765 - val_precision: 0.9487 - val_recall: 0.5604 - val_fmeasure: 0.7009\n",
      "Epoch 303/500\n",
      "7s - loss: 0.0193 - acc: 0.9982 - precision: 0.9983 - recall: 0.9666 - fmeasure: 0.9819 - val_loss: 0.0781 - val_acc: 0.9766 - val_precision: 0.9471 - val_recall: 0.5645 - val_fmeasure: 0.7036\n",
      "Epoch 304/500\n",
      "7s - loss: 0.0192 - acc: 0.9983 - precision: 0.9987 - recall: 0.9664 - fmeasure: 0.9820 - val_loss: 0.0786 - val_acc: 0.9765 - val_precision: 0.9469 - val_recall: 0.5615 - val_fmeasure: 0.7012\n",
      "Epoch 305/500\n",
      "7s - loss: 0.0192 - acc: 0.9982 - precision: 0.9985 - recall: 0.9663 - fmeasure: 0.9819 - val_loss: 0.0785 - val_acc: 0.9765 - val_precision: 0.9453 - val_recall: 0.5636 - val_fmeasure: 0.7023\n",
      "Epoch 306/500\n",
      "7s - loss: 0.0190 - acc: 0.9983 - precision: 0.9990 - recall: 0.9679 - fmeasure: 0.9830 - val_loss: 0.0787 - val_acc: 0.9764 - val_precision: 0.9444 - val_recall: 0.5632 - val_fmeasure: 0.7018\n",
      "Epoch 307/500\n",
      "7s - loss: 0.0191 - acc: 0.9983 - precision: 0.9986 - recall: 0.9665 - fmeasure: 0.9820 - val_loss: 0.0785 - val_acc: 0.9765 - val_precision: 0.9471 - val_recall: 0.5620 - val_fmeasure: 0.7017\n",
      "Epoch 308/500\n",
      "7s - loss: 0.0191 - acc: 0.9982 - precision: 0.9981 - recall: 0.9654 - fmeasure: 0.9812 - val_loss: 0.0784 - val_acc: 0.9765 - val_precision: 0.9479 - val_recall: 0.5623 - val_fmeasure: 0.7020\n",
      "Epoch 309/500\n",
      "7s - loss: 0.0189 - acc: 0.9983 - precision: 0.9982 - recall: 0.9671 - fmeasure: 0.9822 - val_loss: 0.0783 - val_acc: 0.9766 - val_precision: 0.9446 - val_recall: 0.5664 - val_fmeasure: 0.7045\n",
      "Epoch 310/500\n",
      "7s - loss: 0.0189 - acc: 0.9983 - precision: 0.9984 - recall: 0.9680 - fmeasure: 0.9827 - val_loss: 0.0782 - val_acc: 0.9766 - val_precision: 0.9463 - val_recall: 0.5657 - val_fmeasure: 0.7044\n",
      "Epoch 311/500\n",
      "7s - loss: 0.0189 - acc: 0.9984 - precision: 0.9988 - recall: 0.9688 - fmeasure: 0.9833 - val_loss: 0.0778 - val_acc: 0.9767 - val_precision: 0.9442 - val_recall: 0.5676 - val_fmeasure: 0.7052\n",
      "Epoch 312/500\n",
      "7s - loss: 0.0187 - acc: 0.9983 - precision: 0.9985 - recall: 0.9675 - fmeasure: 0.9825 - val_loss: 0.0784 - val_acc: 0.9766 - val_precision: 0.9466 - val_recall: 0.5644 - val_fmeasure: 0.7035\n",
      "Epoch 313/500\n",
      "7s - loss: 0.0189 - acc: 0.9982 - precision: 0.9979 - recall: 0.9671 - fmeasure: 0.9820 - val_loss: 0.0782 - val_acc: 0.9765 - val_precision: 0.9452 - val_recall: 0.5640 - val_fmeasure: 0.7028\n",
      "Epoch 314/500\n",
      "7s - loss: 0.0189 - acc: 0.9981 - precision: 0.9987 - recall: 0.9631 - fmeasure: 0.9803 - val_loss: 0.0778 - val_acc: 0.9767 - val_precision: 0.9447 - val_recall: 0.5677 - val_fmeasure: 0.7055\n",
      "Epoch 315/500\n",
      "7s - loss: 0.0186 - acc: 0.9984 - precision: 0.9983 - recall: 0.9694 - fmeasure: 0.9834 - val_loss: 0.0784 - val_acc: 0.9765 - val_precision: 0.9462 - val_recall: 0.5631 - val_fmeasure: 0.7024\n",
      "Epoch 316/500\n",
      "7s - loss: 0.0188 - acc: 0.9983 - precision: 0.9984 - recall: 0.9673 - fmeasure: 0.9823 - val_loss: 0.0780 - val_acc: 0.9767 - val_precision: 0.9469 - val_recall: 0.5672 - val_fmeasure: 0.7057\n",
      "Epoch 317/500\n",
      "7s - loss: 0.0185 - acc: 0.9984 - precision: 0.9988 - recall: 0.9685 - fmeasure: 0.9832 - val_loss: 0.0778 - val_acc: 0.9767 - val_precision: 0.9453 - val_recall: 0.5669 - val_fmeasure: 0.7051\n",
      "Epoch 318/500\n",
      "7s - loss: 0.0187 - acc: 0.9983 - precision: 0.9988 - recall: 0.9665 - fmeasure: 0.9821 - val_loss: 0.0779 - val_acc: 0.9766 - val_precision: 0.9450 - val_recall: 0.5664 - val_fmeasure: 0.7046\n",
      "Epoch 319/500\n",
      "7s - loss: 0.0186 - acc: 0.9983 - precision: 0.9982 - recall: 0.9682 - fmeasure: 0.9827 - val_loss: 0.0782 - val_acc: 0.9766 - val_precision: 0.9457 - val_recall: 0.5661 - val_fmeasure: 0.7046\n",
      "Epoch 320/500\n",
      "7s - loss: 0.0185 - acc: 0.9984 - precision: 0.9987 - recall: 0.9699 - fmeasure: 0.9838 - val_loss: 0.0783 - val_acc: 0.9765 - val_precision: 0.9462 - val_recall: 0.5635 - val_fmeasure: 0.7026\n",
      "Epoch 321/500\n",
      "7s - loss: 0.0186 - acc: 0.9982 - precision: 0.9986 - recall: 0.9660 - fmeasure: 0.9817 - val_loss: 0.0777 - val_acc: 0.9768 - val_precision: 0.9456 - val_recall: 0.5701 - val_fmeasure: 0.7076\n",
      "Epoch 322/500\n",
      "7s - loss: 0.0184 - acc: 0.9985 - precision: 0.9985 - recall: 0.9708 - fmeasure: 0.9842 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9465 - val_recall: 0.5666 - val_fmeasure: 0.7052\n",
      "Epoch 323/500\n",
      "7s - loss: 0.0184 - acc: 0.9983 - precision: 0.9989 - recall: 0.9677 - fmeasure: 0.9828 - val_loss: 0.0778 - val_acc: 0.9767 - val_precision: 0.9435 - val_recall: 0.5688 - val_fmeasure: 0.7061\n",
      "Epoch 324/500\n",
      "7s - loss: 0.0184 - acc: 0.9983 - precision: 0.9988 - recall: 0.9675 - fmeasure: 0.9826 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9442 - val_recall: 0.5676 - val_fmeasure: 0.7052\n",
      "Epoch 325/500\n",
      "7s - loss: 0.0184 - acc: 0.9983 - precision: 0.9988 - recall: 0.9675 - fmeasure: 0.9826 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9453 - val_recall: 0.5684 - val_fmeasure: 0.7062\n",
      "Epoch 326/500\n",
      "7s - loss: 0.0182 - acc: 0.9985 - precision: 0.9988 - recall: 0.9707 - fmeasure: 0.9844 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9447 - val_recall: 0.5673 - val_fmeasure: 0.7052\n",
      "Epoch 327/500\n",
      "7s - loss: 0.0181 - acc: 0.9985 - precision: 0.9985 - recall: 0.9710 - fmeasure: 0.9843 - val_loss: 0.0780 - val_acc: 0.9766 - val_precision: 0.9449 - val_recall: 0.5659 - val_fmeasure: 0.7042\n",
      "Epoch 328/500\n",
      "7s - loss: 0.0182 - acc: 0.9983 - precision: 0.9978 - recall: 0.9681 - fmeasure: 0.9825 - val_loss: 0.0778 - val_acc: 0.9767 - val_precision: 0.9456 - val_recall: 0.5677 - val_fmeasure: 0.7057\n",
      "Epoch 329/500\n",
      "7s - loss: 0.0180 - acc: 0.9985 - precision: 0.9986 - recall: 0.9714 - fmeasure: 0.9846 - val_loss: 0.0779 - val_acc: 0.9767 - val_precision: 0.9446 - val_recall: 0.5674 - val_fmeasure: 0.7053\n",
      "Epoch 330/500\n",
      "7s - loss: 0.0180 - acc: 0.9985 - precision: 0.9988 - recall: 0.9703 - fmeasure: 0.9841 - val_loss: 0.0778 - val_acc: 0.9766 - val_precision: 0.9449 - val_recall: 0.5669 - val_fmeasure: 0.7050\n",
      "Epoch 331/500\n",
      "7s - loss: 0.0179 - acc: 0.9984 - precision: 0.9988 - recall: 0.9697 - fmeasure: 0.9838 - val_loss: 0.0781 - val_acc: 0.9766 - val_precision: 0.9450 - val_recall: 0.5657 - val_fmeasure: 0.7042\n",
      "Epoch 332/500\n",
      "7s - loss: 0.0180 - acc: 0.9984 - precision: 0.9985 - recall: 0.9702 - fmeasure: 0.9839 - val_loss: 0.0782 - val_acc: 0.9765 - val_precision: 0.9451 - val_recall: 0.5644 - val_fmeasure: 0.7031\n",
      "Epoch 333/500\n",
      "7s - loss: 0.0180 - acc: 0.9984 - precision: 0.9983 - recall: 0.9707 - fmeasure: 0.9841 - val_loss: 0.0783 - val_acc: 0.9766 - val_precision: 0.9456 - val_recall: 0.5657 - val_fmeasure: 0.7042\n",
      "Epoch 334/500\n",
      "7s - loss: 0.0178 - acc: 0.9985 - precision: 0.9988 - recall: 0.9709 - fmeasure: 0.9844 - val_loss: 0.0779 - val_acc: 0.9767 - val_precision: 0.9448 - val_recall: 0.5681 - val_fmeasure: 0.7060\n",
      "Epoch 335/500\n",
      "7s - loss: 0.0178 - acc: 0.9985 - precision: 0.9986 - recall: 0.9706 - fmeasure: 0.9842 - val_loss: 0.0782 - val_acc: 0.9767 - val_precision: 0.9457 - val_recall: 0.5668 - val_fmeasure: 0.7052\n",
      "Epoch 336/500\n",
      "7s - loss: 0.0177 - acc: 0.9985 - precision: 0.9987 - recall: 0.9716 - fmeasure: 0.9847 - val_loss: 0.0782 - val_acc: 0.9766 - val_precision: 0.9455 - val_recall: 0.5652 - val_fmeasure: 0.7039\n",
      "Epoch 337/500\n",
      "7s - loss: 0.0178 - acc: 0.9984 - precision: 0.9985 - recall: 0.9696 - fmeasure: 0.9836 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9457 - val_recall: 0.5676 - val_fmeasure: 0.7057\n",
      "Epoch 338/500\n",
      "7s - loss: 0.0179 - acc: 0.9983 - precision: 0.9985 - recall: 0.9679 - fmeasure: 0.9827 - val_loss: 0.0777 - val_acc: 0.9768 - val_precision: 0.9448 - val_recall: 0.5702 - val_fmeasure: 0.7075\n",
      "Epoch 339/500\n",
      "7s - loss: 0.0177 - acc: 0.9985 - precision: 0.9985 - recall: 0.9721 - fmeasure: 0.9849 - val_loss: 0.0778 - val_acc: 0.9769 - val_precision: 0.9460 - val_recall: 0.5709 - val_fmeasure: 0.7084\n",
      "Epoch 340/500\n",
      "7s - loss: 0.0178 - acc: 0.9985 - precision: 0.9987 - recall: 0.9704 - fmeasure: 0.9840 - val_loss: 0.0778 - val_acc: 0.9769 - val_precision: 0.9458 - val_recall: 0.5709 - val_fmeasure: 0.7083\n",
      "Epoch 341/500\n",
      "7s - loss: 0.0177 - acc: 0.9985 - precision: 0.9981 - recall: 0.9711 - fmeasure: 0.9842 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9448 - val_recall: 0.5714 - val_fmeasure: 0.7085\n",
      "Epoch 342/500\n",
      "7s - loss: 0.0175 - acc: 0.9985 - precision: 0.9989 - recall: 0.9718 - fmeasure: 0.9849 - val_loss: 0.0780 - val_acc: 0.9767 - val_precision: 0.9453 - val_recall: 0.5669 - val_fmeasure: 0.7052\n",
      "Epoch 343/500\n",
      "7s - loss: 0.0175 - acc: 0.9986 - precision: 0.9988 - recall: 0.9722 - fmeasure: 0.9852 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9455 - val_recall: 0.5697 - val_fmeasure: 0.7074\n",
      "Epoch 344/500\n",
      "7s - loss: 0.0176 - acc: 0.9985 - precision: 0.9989 - recall: 0.9704 - fmeasure: 0.9842 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9439 - val_recall: 0.5709 - val_fmeasure: 0.7079\n",
      "Epoch 345/500\n",
      "7s - loss: 0.0174 - acc: 0.9984 - precision: 0.9983 - recall: 0.9700 - fmeasure: 0.9837 - val_loss: 0.0782 - val_acc: 0.9766 - val_precision: 0.9466 - val_recall: 0.5648 - val_fmeasure: 0.7038\n",
      "Epoch 346/500\n",
      "7s - loss: 0.0174 - acc: 0.9984 - precision: 0.9987 - recall: 0.9693 - fmeasure: 0.9836 - val_loss: 0.0779 - val_acc: 0.9768 - val_precision: 0.9458 - val_recall: 0.5697 - val_fmeasure: 0.7075\n",
      "Epoch 347/500\n",
      "7s - loss: 0.0175 - acc: 0.9984 - precision: 0.9983 - recall: 0.9692 - fmeasure: 0.9833 - val_loss: 0.0781 - val_acc: 0.9767 - val_precision: 0.9447 - val_recall: 0.5688 - val_fmeasure: 0.7064\n",
      "Epoch 348/500\n",
      "7s - loss: 0.0175 - acc: 0.9985 - precision: 0.9983 - recall: 0.9709 - fmeasure: 0.9842 - val_loss: 0.0780 - val_acc: 0.9768 - val_precision: 0.9447 - val_recall: 0.5698 - val_fmeasure: 0.7073\n",
      "Epoch 349/500\n",
      "7s - loss: 0.0173 - acc: 0.9986 - precision: 0.9991 - recall: 0.9723 - fmeasure: 0.9853 - val_loss: 0.0775 - val_acc: 0.9768 - val_precision: 0.9444 - val_recall: 0.5716 - val_fmeasure: 0.7084\n",
      "Epoch 350/500\n",
      "7s - loss: 0.0174 - acc: 0.9985 - precision: 0.9989 - recall: 0.9716 - fmeasure: 0.9849 - val_loss: 0.0777 - val_acc: 0.9768 - val_precision: 0.9464 - val_recall: 0.5701 - val_fmeasure: 0.7080\n",
      "Epoch 351/500\n",
      "7s - loss: 0.0173 - acc: 0.9986 - precision: 0.9988 - recall: 0.9726 - fmeasure: 0.9853 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9441 - val_recall: 0.5698 - val_fmeasure: 0.7071\n",
      "Epoch 352/500\n",
      "7s - loss: 0.0172 - acc: 0.9986 - precision: 0.9988 - recall: 0.9723 - fmeasure: 0.9852 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9451 - val_recall: 0.5736 - val_fmeasure: 0.7104\n",
      "Epoch 353/500\n",
      "7s - loss: 0.0172 - acc: 0.9986 - precision: 0.9987 - recall: 0.9730 - fmeasure: 0.9854 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9463 - val_recall: 0.5704 - val_fmeasure: 0.7082\n",
      "Epoch 354/500\n",
      "7s - loss: 0.0171 - acc: 0.9985 - precision: 0.9985 - recall: 0.9724 - fmeasure: 0.9851 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9458 - val_recall: 0.5697 - val_fmeasure: 0.7075\n",
      "Epoch 355/500\n",
      "7s - loss: 0.0171 - acc: 0.9986 - precision: 0.9985 - recall: 0.9738 - fmeasure: 0.9858 - val_loss: 0.0776 - val_acc: 0.9769 - val_precision: 0.9459 - val_recall: 0.5725 - val_fmeasure: 0.7097\n",
      "Epoch 356/500\n",
      "7s - loss: 0.0170 - acc: 0.9986 - precision: 0.9991 - recall: 0.9731 - fmeasure: 0.9858 - val_loss: 0.0779 - val_acc: 0.9767 - val_precision: 0.9464 - val_recall: 0.5677 - val_fmeasure: 0.7062\n",
      "Epoch 357/500\n",
      "7s - loss: 0.0170 - acc: 0.9986 - precision: 0.9988 - recall: 0.9730 - fmeasure: 0.9855 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9441 - val_recall: 0.5724 - val_fmeasure: 0.7091\n",
      "Epoch 358/500\n",
      "7s - loss: 0.0171 - acc: 0.9986 - precision: 0.9988 - recall: 0.9727 - fmeasure: 0.9854 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9466 - val_recall: 0.5692 - val_fmeasure: 0.7072\n",
      "Epoch 359/500\n",
      "7s - loss: 0.0168 - acc: 0.9986 - precision: 0.9989 - recall: 0.9734 - fmeasure: 0.9858 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9444 - val_recall: 0.5724 - val_fmeasure: 0.7092\n",
      "Epoch 360/500\n",
      "7s - loss: 0.0170 - acc: 0.9986 - precision: 0.9989 - recall: 0.9736 - fmeasure: 0.9859 - val_loss: 0.0771 - val_acc: 0.9771 - val_precision: 0.9447 - val_recall: 0.5763 - val_fmeasure: 0.7122\n",
      "Epoch 361/500\n",
      "7s - loss: 0.0169 - acc: 0.9986 - precision: 0.9989 - recall: 0.9741 - fmeasure: 0.9861 - val_loss: 0.0776 - val_acc: 0.9770 - val_precision: 0.9447 - val_recall: 0.5740 - val_fmeasure: 0.7105\n",
      "Epoch 362/500\n",
      "7s - loss: 0.0170 - acc: 0.9985 - precision: 0.9988 - recall: 0.9714 - fmeasure: 0.9846 - val_loss: 0.0774 - val_acc: 0.9769 - val_precision: 0.9440 - val_recall: 0.5734 - val_fmeasure: 0.7099\n",
      "Epoch 363/500\n",
      "7s - loss: 0.0169 - acc: 0.9985 - precision: 0.9986 - recall: 0.9722 - fmeasure: 0.9849 - val_loss: 0.0773 - val_acc: 0.9770 - val_precision: 0.9453 - val_recall: 0.5749 - val_fmeasure: 0.7114\n",
      "Epoch 364/500\n",
      "7s - loss: 0.0167 - acc: 0.9985 - precision: 0.9976 - recall: 0.9730 - fmeasure: 0.9850 - val_loss: 0.0779 - val_acc: 0.9769 - val_precision: 0.9457 - val_recall: 0.5709 - val_fmeasure: 0.7084\n",
      "Epoch 365/500\n",
      "7s - loss: 0.0167 - acc: 0.9986 - precision: 0.9986 - recall: 0.9733 - fmeasure: 0.9856 - val_loss: 0.0779 - val_acc: 0.9769 - val_precision: 0.9451 - val_recall: 0.5718 - val_fmeasure: 0.7089\n",
      "Epoch 366/500\n",
      "7s - loss: 0.0168 - acc: 0.9986 - precision: 0.9988 - recall: 0.9722 - fmeasure: 0.9851 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9452 - val_recall: 0.5717 - val_fmeasure: 0.7088\n",
      "Epoch 367/500\n",
      "7s - loss: 0.0165 - acc: 0.9988 - precision: 0.9989 - recall: 0.9769 - fmeasure: 0.9876 - val_loss: 0.0774 - val_acc: 0.9769 - val_precision: 0.9441 - val_recall: 0.5724 - val_fmeasure: 0.7090\n",
      "Epoch 368/500\n",
      "7s - loss: 0.0167 - acc: 0.9986 - precision: 0.9989 - recall: 0.9725 - fmeasure: 0.9853 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9440 - val_recall: 0.5747 - val_fmeasure: 0.7109\n",
      "Epoch 369/500\n",
      "7s - loss: 0.0167 - acc: 0.9987 - precision: 0.9984 - recall: 0.9751 - fmeasure: 0.9864 - val_loss: 0.0777 - val_acc: 0.9768 - val_precision: 0.9456 - val_recall: 0.5704 - val_fmeasure: 0.7079\n",
      "Epoch 370/500\n",
      "7s - loss: 0.0165 - acc: 0.9987 - precision: 0.9988 - recall: 0.9742 - fmeasure: 0.9862 - val_loss: 0.0775 - val_acc: 0.9769 - val_precision: 0.9457 - val_recall: 0.5717 - val_fmeasure: 0.7089\n",
      "Epoch 371/500\n",
      "7s - loss: 0.0165 - acc: 0.9986 - precision: 0.9988 - recall: 0.9739 - fmeasure: 0.9860 - val_loss: 0.0774 - val_acc: 0.9769 - val_precision: 0.9445 - val_recall: 0.5730 - val_fmeasure: 0.7096\n",
      "Epoch 372/500\n",
      "7s - loss: 0.0166 - acc: 0.9986 - precision: 0.9989 - recall: 0.9737 - fmeasure: 0.9860 - val_loss: 0.0772 - val_acc: 0.9770 - val_precision: 0.9440 - val_recall: 0.5751 - val_fmeasure: 0.7112\n",
      "Epoch 373/500\n",
      "7s - loss: 0.0164 - acc: 0.9986 - precision: 0.9991 - recall: 0.9730 - fmeasure: 0.9857 - val_loss: 0.0778 - val_acc: 0.9768 - val_precision: 0.9438 - val_recall: 0.5706 - val_fmeasure: 0.7077\n",
      "Epoch 374/500\n",
      "7s - loss: 0.0165 - acc: 0.9986 - precision: 0.9982 - recall: 0.9747 - fmeasure: 0.9861 - val_loss: 0.0777 - val_acc: 0.9769 - val_precision: 0.9449 - val_recall: 0.5722 - val_fmeasure: 0.7092\n",
      "Epoch 375/500\n",
      "7s - loss: 0.0163 - acc: 0.9986 - precision: 0.9991 - recall: 0.9737 - fmeasure: 0.9860 - val_loss: 0.0775 - val_acc: 0.9770 - val_precision: 0.9454 - val_recall: 0.5729 - val_fmeasure: 0.7099\n",
      "Epoch 376/500\n",
      "7s - loss: 0.0164 - acc: 0.9987 - precision: 0.9988 - recall: 0.9743 - fmeasure: 0.9862 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9447 - val_recall: 0.5738 - val_fmeasure: 0.7104\n",
      "Epoch 377/500\n",
      "7s - loss: 0.0163 - acc: 0.9986 - precision: 0.9985 - recall: 0.9742 - fmeasure: 0.9860 - val_loss: 0.0775 - val_acc: 0.9769 - val_precision: 0.9453 - val_recall: 0.5717 - val_fmeasure: 0.7089\n",
      "Epoch 378/500\n",
      "7s - loss: 0.0164 - acc: 0.9986 - precision: 0.9988 - recall: 0.9730 - fmeasure: 0.9855 - val_loss: 0.0772 - val_acc: 0.9770 - val_precision: 0.9451 - val_recall: 0.5743 - val_fmeasure: 0.7110\n",
      "Epoch 379/500\n",
      "7s - loss: 0.0161 - acc: 0.9988 - precision: 0.9989 - recall: 0.9775 - fmeasure: 0.9879 - val_loss: 0.0775 - val_acc: 0.9770 - val_precision: 0.9461 - val_recall: 0.5737 - val_fmeasure: 0.7106\n",
      "Epoch 380/500\n",
      "7s - loss: 0.0162 - acc: 0.9987 - precision: 0.9989 - recall: 0.9748 - fmeasure: 0.9865 - val_loss: 0.0774 - val_acc: 0.9771 - val_precision: 0.9442 - val_recall: 0.5759 - val_fmeasure: 0.7119\n",
      "Epoch 381/500\n",
      "7s - loss: 0.0162 - acc: 0.9987 - precision: 0.9987 - recall: 0.9752 - fmeasure: 0.9866 - val_loss: 0.0772 - val_acc: 0.9770 - val_precision: 0.9435 - val_recall: 0.5763 - val_fmeasure: 0.7119\n",
      "Epoch 382/500\n",
      "7s - loss: 0.0161 - acc: 0.9988 - precision: 0.9992 - recall: 0.9760 - fmeasure: 0.9873 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9457 - val_recall: 0.5757 - val_fmeasure: 0.7121\n",
      "Epoch 383/500\n",
      "7s - loss: 0.0163 - acc: 0.9987 - precision: 0.9987 - recall: 0.9752 - fmeasure: 0.9866 - val_loss: 0.0772 - val_acc: 0.9771 - val_precision: 0.9448 - val_recall: 0.5757 - val_fmeasure: 0.7119\n",
      "Epoch 384/500\n",
      "7s - loss: 0.0162 - acc: 0.9987 - precision: 0.9988 - recall: 0.9751 - fmeasure: 0.9866 - val_loss: 0.0770 - val_acc: 0.9772 - val_precision: 0.9444 - val_recall: 0.5795 - val_fmeasure: 0.7148\n",
      "Epoch 385/500\n",
      "7s - loss: 0.0161 - acc: 0.9987 - precision: 0.9986 - recall: 0.9751 - fmeasure: 0.9865 - val_loss: 0.0776 - val_acc: 0.9770 - val_precision: 0.9451 - val_recall: 0.5736 - val_fmeasure: 0.7103\n",
      "Epoch 386/500\n",
      "7s - loss: 0.0159 - acc: 0.9988 - precision: 0.9988 - recall: 0.9781 - fmeasure: 0.9881 - val_loss: 0.0771 - val_acc: 0.9770 - val_precision: 0.9449 - val_recall: 0.5743 - val_fmeasure: 0.7109\n",
      "Epoch 387/500\n",
      "7s - loss: 0.0159 - acc: 0.9988 - precision: 0.9989 - recall: 0.9766 - fmeasure: 0.9875 - val_loss: 0.0777 - val_acc: 0.9768 - val_precision: 0.9449 - val_recall: 0.5706 - val_fmeasure: 0.7080\n",
      "Epoch 388/500\n",
      "7s - loss: 0.0160 - acc: 0.9987 - precision: 0.9985 - recall: 0.9752 - fmeasure: 0.9865 - val_loss: 0.0775 - val_acc: 0.9771 - val_precision: 0.9452 - val_recall: 0.5758 - val_fmeasure: 0.7121\n",
      "Epoch 389/500\n",
      "7s - loss: 0.0160 - acc: 0.9988 - precision: 0.9991 - recall: 0.9760 - fmeasure: 0.9872 - val_loss: 0.0775 - val_acc: 0.9770 - val_precision: 0.9447 - val_recall: 0.5747 - val_fmeasure: 0.7111\n",
      "Epoch 390/500\n",
      "7s - loss: 0.0160 - acc: 0.9987 - precision: 0.9984 - recall: 0.9753 - fmeasure: 0.9865 - val_loss: 0.0770 - val_acc: 0.9771 - val_precision: 0.9437 - val_recall: 0.5773 - val_fmeasure: 0.7128\n",
      "Epoch 391/500\n",
      "7s - loss: 0.0159 - acc: 0.9987 - precision: 0.9991 - recall: 0.9755 - fmeasure: 0.9870 - val_loss: 0.0770 - val_acc: 0.9772 - val_precision: 0.9435 - val_recall: 0.5794 - val_fmeasure: 0.7143\n",
      "Epoch 392/500\n",
      "7s - loss: 0.0158 - acc: 0.9988 - precision: 0.9992 - recall: 0.9767 - fmeasure: 0.9876 - val_loss: 0.0772 - val_acc: 0.9770 - val_precision: 0.9441 - val_recall: 0.5755 - val_fmeasure: 0.7113\n",
      "Epoch 393/500\n",
      "7s - loss: 0.0159 - acc: 0.9987 - precision: 0.9991 - recall: 0.9752 - fmeasure: 0.9868 - val_loss: 0.0769 - val_acc: 0.9771 - val_precision: 0.9454 - val_recall: 0.5765 - val_fmeasure: 0.7126\n",
      "Epoch 394/500\n",
      "7s - loss: 0.0158 - acc: 0.9987 - precision: 0.9982 - recall: 0.9756 - fmeasure: 0.9865 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9448 - val_recall: 0.5757 - val_fmeasure: 0.7118\n",
      "Epoch 395/500\n",
      "7s - loss: 0.0157 - acc: 0.9987 - precision: 0.9990 - recall: 0.9756 - fmeasure: 0.9870 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9450 - val_recall: 0.5746 - val_fmeasure: 0.7111\n",
      "Epoch 396/500\n",
      "7s - loss: 0.0158 - acc: 0.9987 - precision: 0.9992 - recall: 0.9750 - fmeasure: 0.9867 - val_loss: 0.0772 - val_acc: 0.9771 - val_precision: 0.9451 - val_recall: 0.5762 - val_fmeasure: 0.7125\n",
      "Epoch 397/500\n",
      "7s - loss: 0.0159 - acc: 0.9987 - precision: 0.9981 - recall: 0.9758 - fmeasure: 0.9866 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9434 - val_recall: 0.5785 - val_fmeasure: 0.7136\n",
      "Epoch 398/500\n",
      "7s - loss: 0.0158 - acc: 0.9987 - precision: 0.9990 - recall: 0.9745 - fmeasure: 0.9864 - val_loss: 0.0773 - val_acc: 0.9770 - val_precision: 0.9462 - val_recall: 0.5742 - val_fmeasure: 0.7111\n",
      "Epoch 399/500\n",
      "7s - loss: 0.0157 - acc: 0.9988 - precision: 0.9991 - recall: 0.9764 - fmeasure: 0.9874 - val_loss: 0.0769 - val_acc: 0.9771 - val_precision: 0.9434 - val_recall: 0.5770 - val_fmeasure: 0.7124\n",
      "Epoch 400/500\n",
      "7s - loss: 0.0157 - acc: 0.9988 - precision: 0.9985 - recall: 0.9767 - fmeasure: 0.9873 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9446 - val_recall: 0.5746 - val_fmeasure: 0.7110\n",
      "Epoch 401/500\n",
      "7s - loss: 0.0156 - acc: 0.9988 - precision: 0.9990 - recall: 0.9777 - fmeasure: 0.9881 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9453 - val_recall: 0.5765 - val_fmeasure: 0.7126\n",
      "Epoch 402/500\n",
      "7s - loss: 0.0156 - acc: 0.9988 - precision: 0.9988 - recall: 0.9765 - fmeasure: 0.9873 - val_loss: 0.0770 - val_acc: 0.9772 - val_precision: 0.9437 - val_recall: 0.5797 - val_fmeasure: 0.7147\n",
      "Epoch 403/500\n",
      "7s - loss: 0.0156 - acc: 0.9987 - precision: 0.9986 - recall: 0.9763 - fmeasure: 0.9871 - val_loss: 0.0768 - val_acc: 0.9772 - val_precision: 0.9443 - val_recall: 0.5786 - val_fmeasure: 0.7139\n",
      "Epoch 404/500\n",
      "7s - loss: 0.0154 - acc: 0.9989 - precision: 0.9989 - recall: 0.9785 - fmeasure: 0.9884 - val_loss: 0.0774 - val_acc: 0.9771 - val_precision: 0.9450 - val_recall: 0.5759 - val_fmeasure: 0.7121\n",
      "Epoch 405/500\n",
      "7s - loss: 0.0155 - acc: 0.9988 - precision: 0.9981 - recall: 0.9774 - fmeasure: 0.9875 - val_loss: 0.0772 - val_acc: 0.9771 - val_precision: 0.9465 - val_recall: 0.5757 - val_fmeasure: 0.7123\n",
      "Epoch 406/500\n",
      "7s - loss: 0.0154 - acc: 0.9989 - precision: 0.9991 - recall: 0.9781 - fmeasure: 0.9883 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9444 - val_recall: 0.5775 - val_fmeasure: 0.7131\n",
      "Epoch 407/500\n",
      "7s - loss: 0.0154 - acc: 0.9988 - precision: 0.9988 - recall: 0.9778 - fmeasure: 0.9880 - val_loss: 0.0772 - val_acc: 0.9771 - val_precision: 0.9438 - val_recall: 0.5774 - val_fmeasure: 0.7129\n",
      "Epoch 408/500\n",
      "7s - loss: 0.0154 - acc: 0.9988 - precision: 0.9992 - recall: 0.9776 - fmeasure: 0.9881 - val_loss: 0.0767 - val_acc: 0.9772 - val_precision: 0.9442 - val_recall: 0.5799 - val_fmeasure: 0.7149\n",
      "Epoch 409/500\n",
      "7s - loss: 0.0154 - acc: 0.9989 - precision: 0.9989 - recall: 0.9787 - fmeasure: 0.9885 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9432 - val_recall: 0.5814 - val_fmeasure: 0.7157\n",
      "Epoch 410/500\n",
      "7s - loss: 0.0154 - acc: 0.9988 - precision: 0.9986 - recall: 0.9783 - fmeasure: 0.9882 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9441 - val_recall: 0.5797 - val_fmeasure: 0.7147\n",
      "Epoch 411/500\n",
      "7s - loss: 0.0154 - acc: 0.9988 - precision: 0.9991 - recall: 0.9762 - fmeasure: 0.9873 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9446 - val_recall: 0.5783 - val_fmeasure: 0.7138\n",
      "Epoch 412/500\n",
      "7s - loss: 0.0154 - acc: 0.9988 - precision: 0.9984 - recall: 0.9773 - fmeasure: 0.9876 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9443 - val_recall: 0.5810 - val_fmeasure: 0.7159\n",
      "Epoch 413/500\n",
      "7s - loss: 0.0153 - acc: 0.9987 - precision: 0.9985 - recall: 0.9765 - fmeasure: 0.9872 - val_loss: 0.0774 - val_acc: 0.9770 - val_precision: 0.9444 - val_recall: 0.5757 - val_fmeasure: 0.7116\n",
      "Epoch 414/500\n",
      "7s - loss: 0.0152 - acc: 0.9988 - precision: 0.9988 - recall: 0.9768 - fmeasure: 0.9874 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9459 - val_recall: 0.5779 - val_fmeasure: 0.7139\n",
      "Epoch 415/500\n",
      "7s - loss: 0.0153 - acc: 0.9988 - precision: 0.9989 - recall: 0.9769 - fmeasure: 0.9876 - val_loss: 0.0770 - val_acc: 0.9772 - val_precision: 0.9455 - val_recall: 0.5782 - val_fmeasure: 0.7139\n",
      "Epoch 416/500\n",
      "7s - loss: 0.0152 - acc: 0.9989 - precision: 0.9985 - recall: 0.9795 - fmeasure: 0.9887 - val_loss: 0.0768 - val_acc: 0.9773 - val_precision: 0.9454 - val_recall: 0.5793 - val_fmeasure: 0.7149\n",
      "Epoch 417/500\n",
      "7s - loss: 0.0151 - acc: 0.9989 - precision: 0.9990 - recall: 0.9782 - fmeasure: 0.9883 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9445 - val_recall: 0.5798 - val_fmeasure: 0.7151\n",
      "Epoch 418/500\n",
      "7s - loss: 0.0152 - acc: 0.9988 - precision: 0.9986 - recall: 0.9782 - fmeasure: 0.9881 - val_loss: 0.0774 - val_acc: 0.9771 - val_precision: 0.9459 - val_recall: 0.5757 - val_fmeasure: 0.7122\n",
      "Epoch 419/500\n",
      "7s - loss: 0.0151 - acc: 0.9988 - precision: 0.9989 - recall: 0.9780 - fmeasure: 0.9881 - val_loss: 0.0772 - val_acc: 0.9771 - val_precision: 0.9440 - val_recall: 0.5765 - val_fmeasure: 0.7124\n",
      "Epoch 420/500\n",
      "7s - loss: 0.0151 - acc: 0.9988 - precision: 0.9983 - recall: 0.9775 - fmeasure: 0.9876 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9437 - val_recall: 0.5798 - val_fmeasure: 0.7147\n",
      "Epoch 421/500\n",
      "7s - loss: 0.0150 - acc: 0.9989 - precision: 0.9988 - recall: 0.9789 - fmeasure: 0.9885 - val_loss: 0.0767 - val_acc: 0.9772 - val_precision: 0.9437 - val_recall: 0.5797 - val_fmeasure: 0.7146\n",
      "Epoch 422/500\n",
      "7s - loss: 0.0149 - acc: 0.9989 - precision: 0.9991 - recall: 0.9798 - fmeasure: 0.9892 - val_loss: 0.0769 - val_acc: 0.9772 - val_precision: 0.9449 - val_recall: 0.5789 - val_fmeasure: 0.7143\n",
      "Epoch 423/500\n",
      "7s - loss: 0.0150 - acc: 0.9990 - precision: 0.9989 - recall: 0.9801 - fmeasure: 0.9893 - val_loss: 0.0775 - val_acc: 0.9771 - val_precision: 0.9461 - val_recall: 0.5758 - val_fmeasure: 0.7124\n",
      "Epoch 424/500\n",
      "7s - loss: 0.0149 - acc: 0.9989 - precision: 0.9990 - recall: 0.9790 - fmeasure: 0.9887 - val_loss: 0.0770 - val_acc: 0.9773 - val_precision: 0.9452 - val_recall: 0.5813 - val_fmeasure: 0.7165\n",
      "Epoch 425/500\n",
      "7s - loss: 0.0149 - acc: 0.9988 - precision: 0.9989 - recall: 0.9780 - fmeasure: 0.9882 - val_loss: 0.0768 - val_acc: 0.9773 - val_precision: 0.9436 - val_recall: 0.5805 - val_fmeasure: 0.7152\n",
      "Epoch 426/500\n",
      "7s - loss: 0.0149 - acc: 0.9989 - precision: 0.9989 - recall: 0.9796 - fmeasure: 0.9890 - val_loss: 0.0771 - val_acc: 0.9771 - val_precision: 0.9443 - val_recall: 0.5777 - val_fmeasure: 0.7133\n",
      "Epoch 427/500\n",
      "7s - loss: 0.0148 - acc: 0.9989 - precision: 0.9986 - recall: 0.9798 - fmeasure: 0.9889 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9440 - val_recall: 0.5775 - val_fmeasure: 0.7131\n",
      "Epoch 428/500\n",
      "7s - loss: 0.0149 - acc: 0.9988 - precision: 0.9987 - recall: 0.9768 - fmeasure: 0.9874 - val_loss: 0.0770 - val_acc: 0.9772 - val_precision: 0.9442 - val_recall: 0.5799 - val_fmeasure: 0.7150\n",
      "Epoch 429/500\n",
      "7s - loss: 0.0147 - acc: 0.9989 - precision: 0.9989 - recall: 0.9786 - fmeasure: 0.9884 - val_loss: 0.0775 - val_acc: 0.9771 - val_precision: 0.9457 - val_recall: 0.5754 - val_fmeasure: 0.7119\n",
      "Epoch 430/500\n",
      "7s - loss: 0.0149 - acc: 0.9988 - precision: 0.9985 - recall: 0.9768 - fmeasure: 0.9874 - val_loss: 0.0772 - val_acc: 0.9772 - val_precision: 0.9444 - val_recall: 0.5783 - val_fmeasure: 0.7137\n",
      "Epoch 431/500\n",
      "7s - loss: 0.0149 - acc: 0.9989 - precision: 0.9989 - recall: 0.9787 - fmeasure: 0.9885 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9446 - val_recall: 0.5805 - val_fmeasure: 0.7156\n",
      "Epoch 432/500\n",
      "7s - loss: 0.0147 - acc: 0.9989 - precision: 0.9988 - recall: 0.9789 - fmeasure: 0.9885 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9448 - val_recall: 0.5805 - val_fmeasure: 0.7157\n",
      "Epoch 433/500\n",
      "7s - loss: 0.0147 - acc: 0.9989 - precision: 0.9989 - recall: 0.9785 - fmeasure: 0.9884 - val_loss: 0.0773 - val_acc: 0.9773 - val_precision: 0.9447 - val_recall: 0.5797 - val_fmeasure: 0.7150\n",
      "Epoch 434/500\n",
      "7s - loss: 0.0147 - acc: 0.9989 - precision: 0.9990 - recall: 0.9782 - fmeasure: 0.9883 - val_loss: 0.0772 - val_acc: 0.9773 - val_precision: 0.9439 - val_recall: 0.5807 - val_fmeasure: 0.7155\n",
      "Epoch 435/500\n",
      "7s - loss: 0.0147 - acc: 0.9988 - precision: 0.9981 - recall: 0.9782 - fmeasure: 0.9879 - val_loss: 0.0773 - val_acc: 0.9771 - val_precision: 0.9451 - val_recall: 0.5773 - val_fmeasure: 0.7132\n",
      "Epoch 436/500\n",
      "7s - loss: 0.0147 - acc: 0.9988 - precision: 0.9989 - recall: 0.9779 - fmeasure: 0.9881 - val_loss: 0.0766 - val_acc: 0.9773 - val_precision: 0.9422 - val_recall: 0.5819 - val_fmeasure: 0.7159\n",
      "Epoch 437/500\n",
      "7s - loss: 0.0146 - acc: 0.9989 - precision: 0.9991 - recall: 0.9786 - fmeasure: 0.9886 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9436 - val_recall: 0.5820 - val_fmeasure: 0.7165\n",
      "Epoch 438/500\n",
      "7s - loss: 0.0148 - acc: 0.9989 - precision: 0.9989 - recall: 0.9786 - fmeasure: 0.9885 - val_loss: 0.0768 - val_acc: 0.9773 - val_precision: 0.9435 - val_recall: 0.5823 - val_fmeasure: 0.7165\n",
      "Epoch 439/500\n",
      "7s - loss: 0.0146 - acc: 0.9989 - precision: 0.9989 - recall: 0.9798 - fmeasure: 0.9891 - val_loss: 0.0764 - val_acc: 0.9775 - val_precision: 0.9429 - val_recall: 0.5856 - val_fmeasure: 0.7189\n",
      "Epoch 440/500\n",
      "7s - loss: 0.0144 - acc: 0.9990 - precision: 0.9990 - recall: 0.9812 - fmeasure: 0.9899 - val_loss: 0.0767 - val_acc: 0.9773 - val_precision: 0.9443 - val_recall: 0.5817 - val_fmeasure: 0.7165\n",
      "Epoch 441/500\n",
      "7s - loss: 0.0145 - acc: 0.9989 - precision: 0.9989 - recall: 0.9799 - fmeasure: 0.9892 - val_loss: 0.0771 - val_acc: 0.9773 - val_precision: 0.9432 - val_recall: 0.5807 - val_fmeasure: 0.7154\n",
      "Epoch 442/500\n",
      "7s - loss: 0.0145 - acc: 0.9989 - precision: 0.9988 - recall: 0.9794 - fmeasure: 0.9888 - val_loss: 0.0771 - val_acc: 0.9772 - val_precision: 0.9412 - val_recall: 0.5803 - val_fmeasure: 0.7144\n",
      "Epoch 443/500\n",
      "7s - loss: 0.0145 - acc: 0.9989 - precision: 0.9991 - recall: 0.9791 - fmeasure: 0.9888 - val_loss: 0.0766 - val_acc: 0.9773 - val_precision: 0.9429 - val_recall: 0.5817 - val_fmeasure: 0.7159\n",
      "Epoch 444/500\n",
      "7s - loss: 0.0145 - acc: 0.9989 - precision: 0.9989 - recall: 0.9791 - fmeasure: 0.9887 - val_loss: 0.0764 - val_acc: 0.9774 - val_precision: 0.9436 - val_recall: 0.5832 - val_fmeasure: 0.7174\n",
      "Epoch 445/500\n",
      "7s - loss: 0.0146 - acc: 0.9988 - precision: 0.9988 - recall: 0.9781 - fmeasure: 0.9882 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9411 - val_recall: 0.5848 - val_fmeasure: 0.7178\n",
      "Epoch 446/500\n",
      "7s - loss: 0.0143 - acc: 0.9990 - precision: 0.9990 - recall: 0.9811 - fmeasure: 0.9898 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9456 - val_recall: 0.5794 - val_fmeasure: 0.7149\n",
      "Epoch 447/500\n",
      "7s - loss: 0.0144 - acc: 0.9989 - precision: 0.9989 - recall: 0.9791 - fmeasure: 0.9888 - val_loss: 0.0766 - val_acc: 0.9773 - val_precision: 0.9437 - val_recall: 0.5820 - val_fmeasure: 0.7165\n",
      "Epoch 448/500\n",
      "7s - loss: 0.0143 - acc: 0.9989 - precision: 0.9987 - recall: 0.9799 - fmeasure: 0.9891 - val_loss: 0.0768 - val_acc: 0.9773 - val_precision: 0.9446 - val_recall: 0.5801 - val_fmeasure: 0.7153\n",
      "Epoch 449/500\n",
      "7s - loss: 0.0144 - acc: 0.9989 - precision: 0.9987 - recall: 0.9787 - fmeasure: 0.9884 - val_loss: 0.0765 - val_acc: 0.9774 - val_precision: 0.9433 - val_recall: 0.5839 - val_fmeasure: 0.7178\n",
      "Epoch 450/500\n",
      "7s - loss: 0.0143 - acc: 0.9989 - precision: 0.9988 - recall: 0.9797 - fmeasure: 0.9890 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9441 - val_recall: 0.5807 - val_fmeasure: 0.7156\n",
      "Epoch 451/500\n",
      "7s - loss: 0.0142 - acc: 0.9989 - precision: 0.9989 - recall: 0.9801 - fmeasure: 0.9892 - val_loss: 0.0770 - val_acc: 0.9773 - val_precision: 0.9438 - val_recall: 0.5807 - val_fmeasure: 0.7156\n",
      "Epoch 452/500\n",
      "7s - loss: 0.0143 - acc: 0.9989 - precision: 0.9988 - recall: 0.9796 - fmeasure: 0.9889 - val_loss: 0.0771 - val_acc: 0.9772 - val_precision: 0.9436 - val_recall: 0.5799 - val_fmeasure: 0.7150\n",
      "Epoch 453/500\n",
      "7s - loss: 0.0142 - acc: 0.9989 - precision: 0.9993 - recall: 0.9796 - fmeasure: 0.9892 - val_loss: 0.0766 - val_acc: 0.9774 - val_precision: 0.9443 - val_recall: 0.5827 - val_fmeasure: 0.7172\n",
      "Epoch 454/500\n",
      "7s - loss: 0.0144 - acc: 0.9988 - precision: 0.9987 - recall: 0.9778 - fmeasure: 0.9880 - val_loss: 0.0766 - val_acc: 0.9774 - val_precision: 0.9430 - val_recall: 0.5842 - val_fmeasure: 0.7180\n",
      "Epoch 455/500\n",
      "7s - loss: 0.0143 - acc: 0.9989 - precision: 0.9992 - recall: 0.9778 - fmeasure: 0.9882 - val_loss: 0.0765 - val_acc: 0.9775 - val_precision: 0.9435 - val_recall: 0.5855 - val_fmeasure: 0.7190\n",
      "Epoch 456/500\n",
      "7s - loss: 0.0141 - acc: 0.9990 - precision: 0.9992 - recall: 0.9803 - fmeasure: 0.9895 - val_loss: 0.0766 - val_acc: 0.9775 - val_precision: 0.9440 - val_recall: 0.5858 - val_fmeasure: 0.7195\n",
      "Epoch 457/500\n",
      "7s - loss: 0.0141 - acc: 0.9990 - precision: 0.9988 - recall: 0.9808 - fmeasure: 0.9895 - val_loss: 0.0763 - val_acc: 0.9776 - val_precision: 0.9420 - val_recall: 0.5884 - val_fmeasure: 0.7210\n",
      "Epoch 458/500\n",
      "7s - loss: 0.0142 - acc: 0.9989 - precision: 0.9988 - recall: 0.9794 - fmeasure: 0.9889 - val_loss: 0.0766 - val_acc: 0.9775 - val_precision: 0.9437 - val_recall: 0.5854 - val_fmeasure: 0.7189\n",
      "Epoch 459/500\n",
      "7s - loss: 0.0140 - acc: 0.9990 - precision: 0.9992 - recall: 0.9806 - fmeasure: 0.9897 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9435 - val_recall: 0.5832 - val_fmeasure: 0.7173\n",
      "Epoch 460/500\n",
      "7s - loss: 0.0141 - acc: 0.9989 - precision: 0.9986 - recall: 0.9798 - fmeasure: 0.9889 - val_loss: 0.0765 - val_acc: 0.9774 - val_precision: 0.9428 - val_recall: 0.5852 - val_fmeasure: 0.7187\n",
      "Epoch 461/500\n",
      "7s - loss: 0.0141 - acc: 0.9989 - precision: 0.9989 - recall: 0.9794 - fmeasure: 0.9888 - val_loss: 0.0765 - val_acc: 0.9774 - val_precision: 0.9433 - val_recall: 0.5834 - val_fmeasure: 0.7174\n",
      "Epoch 462/500\n",
      "7s - loss: 0.0140 - acc: 0.9991 - precision: 0.9990 - recall: 0.9821 - fmeasure: 0.9903 - val_loss: 0.0768 - val_acc: 0.9773 - val_precision: 0.9444 - val_recall: 0.5813 - val_fmeasure: 0.7161\n",
      "Epoch 463/500\n",
      "7s - loss: 0.0140 - acc: 0.9990 - precision: 0.9989 - recall: 0.9806 - fmeasure: 0.9896 - val_loss: 0.0770 - val_acc: 0.9774 - val_precision: 0.9457 - val_recall: 0.5814 - val_fmeasure: 0.7165\n",
      "Epoch 464/500\n",
      "7s - loss: 0.0140 - acc: 0.9990 - precision: 0.9988 - recall: 0.9819 - fmeasure: 0.9901 - val_loss: 0.0771 - val_acc: 0.9773 - val_precision: 0.9453 - val_recall: 0.5795 - val_fmeasure: 0.7150\n",
      "Epoch 465/500\n",
      "7s - loss: 0.0140 - acc: 0.9990 - precision: 0.9992 - recall: 0.9806 - fmeasure: 0.9896 - val_loss: 0.0764 - val_acc: 0.9775 - val_precision: 0.9431 - val_recall: 0.5871 - val_fmeasure: 0.7201\n",
      "Epoch 466/500\n",
      "7s - loss: 0.0138 - acc: 0.9991 - precision: 0.9989 - recall: 0.9822 - fmeasure: 0.9904 - val_loss: 0.0765 - val_acc: 0.9775 - val_precision: 0.9448 - val_recall: 0.5844 - val_fmeasure: 0.7187\n",
      "Epoch 467/500\n",
      "7s - loss: 0.0139 - acc: 0.9990 - precision: 0.9988 - recall: 0.9806 - fmeasure: 0.9894 - val_loss: 0.0764 - val_acc: 0.9774 - val_precision: 0.9427 - val_recall: 0.5854 - val_fmeasure: 0.7188\n",
      "Epoch 468/500\n",
      "7s - loss: 0.0139 - acc: 0.9990 - precision: 0.9986 - recall: 0.9811 - fmeasure: 0.9896 - val_loss: 0.0770 - val_acc: 0.9773 - val_precision: 0.9459 - val_recall: 0.5805 - val_fmeasure: 0.7159\n",
      "Epoch 469/500\n",
      "7s - loss: 0.0140 - acc: 0.9989 - precision: 0.9986 - recall: 0.9799 - fmeasure: 0.9890 - val_loss: 0.0769 - val_acc: 0.9773 - val_precision: 0.9433 - val_recall: 0.5820 - val_fmeasure: 0.7163\n",
      "Epoch 470/500\n",
      "7s - loss: 0.0139 - acc: 0.9990 - precision: 0.9992 - recall: 0.9807 - fmeasure: 0.9897 - val_loss: 0.0767 - val_acc: 0.9775 - val_precision: 0.9445 - val_recall: 0.5843 - val_fmeasure: 0.7185\n",
      "Epoch 471/500\n",
      "7s - loss: 0.0139 - acc: 0.9989 - precision: 0.9988 - recall: 0.9794 - fmeasure: 0.9889 - val_loss: 0.0768 - val_acc: 0.9774 - val_precision: 0.9432 - val_recall: 0.5850 - val_fmeasure: 0.7186\n",
      "Epoch 472/500\n",
      "7s - loss: 0.0138 - acc: 0.9990 - precision: 0.9986 - recall: 0.9814 - fmeasure: 0.9898 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9435 - val_recall: 0.5835 - val_fmeasure: 0.7175\n",
      "Epoch 473/500\n",
      "7s - loss: 0.0138 - acc: 0.9990 - precision: 0.9988 - recall: 0.9821 - fmeasure: 0.9902 - val_loss: 0.0770 - val_acc: 0.9773 - val_precision: 0.9443 - val_recall: 0.5819 - val_fmeasure: 0.7166\n",
      "Epoch 474/500\n",
      "7s - loss: 0.0138 - acc: 0.9990 - precision: 0.9988 - recall: 0.9805 - fmeasure: 0.9894 - val_loss: 0.0768 - val_acc: 0.9774 - val_precision: 0.9433 - val_recall: 0.5839 - val_fmeasure: 0.7177\n",
      "Epoch 475/500\n",
      "7s - loss: 0.0138 - acc: 0.9990 - precision: 0.9988 - recall: 0.9810 - fmeasure: 0.9896 - val_loss: 0.0766 - val_acc: 0.9774 - val_precision: 0.9445 - val_recall: 0.5839 - val_fmeasure: 0.7182\n",
      "Epoch 476/500\n",
      "7s - loss: 0.0137 - acc: 0.9990 - precision: 0.9989 - recall: 0.9815 - fmeasure: 0.9900 - val_loss: 0.0764 - val_acc: 0.9775 - val_precision: 0.9425 - val_recall: 0.5870 - val_fmeasure: 0.7198\n",
      "Epoch 477/500\n",
      "7s - loss: 0.0136 - acc: 0.9991 - precision: 0.9993 - recall: 0.9832 - fmeasure: 0.9911 - val_loss: 0.0769 - val_acc: 0.9774 - val_precision: 0.9446 - val_recall: 0.5824 - val_fmeasure: 0.7169\n",
      "Epoch 478/500\n",
      "7s - loss: 0.0138 - acc: 0.9990 - precision: 0.9988 - recall: 0.9818 - fmeasure: 0.9901 - val_loss: 0.0766 - val_acc: 0.9775 - val_precision: 0.9441 - val_recall: 0.5851 - val_fmeasure: 0.7190\n",
      "Epoch 479/500\n",
      "7s - loss: 0.0136 - acc: 0.9991 - precision: 0.9985 - recall: 0.9828 - fmeasure: 0.9904 - val_loss: 0.0769 - val_acc: 0.9774 - val_precision: 0.9449 - val_recall: 0.5818 - val_fmeasure: 0.7165\n",
      "Epoch 480/500\n",
      "7s - loss: 0.0137 - acc: 0.9990 - precision: 0.9986 - recall: 0.9813 - fmeasure: 0.9897 - val_loss: 0.0764 - val_acc: 0.9775 - val_precision: 0.9434 - val_recall: 0.5867 - val_fmeasure: 0.7199\n",
      "Epoch 481/500\n",
      "7s - loss: 0.0136 - acc: 0.9991 - precision: 0.9985 - recall: 0.9836 - fmeasure: 0.9908 - val_loss: 0.0769 - val_acc: 0.9774 - val_precision: 0.9450 - val_recall: 0.5830 - val_fmeasure: 0.7174\n",
      "Epoch 482/500\n",
      "7s - loss: 0.0135 - acc: 0.9991 - precision: 0.9990 - recall: 0.9831 - fmeasure: 0.9909 - val_loss: 0.0767 - val_acc: 0.9775 - val_precision: 0.9432 - val_recall: 0.5855 - val_fmeasure: 0.7189\n",
      "Epoch 483/500\n",
      "7s - loss: 0.0135 - acc: 0.9990 - precision: 0.9988 - recall: 0.9813 - fmeasure: 0.9898 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9427 - val_recall: 0.5850 - val_fmeasure: 0.7185\n",
      "Epoch 484/500\n",
      "7s - loss: 0.0135 - acc: 0.9991 - precision: 0.9989 - recall: 0.9824 - fmeasure: 0.9904 - val_loss: 0.0771 - val_acc: 0.9773 - val_precision: 0.9450 - val_recall: 0.5815 - val_fmeasure: 0.7163\n",
      "Epoch 485/500\n",
      "7s - loss: 0.0136 - acc: 0.9990 - precision: 0.9987 - recall: 0.9816 - fmeasure: 0.9900 - val_loss: 0.0768 - val_acc: 0.9774 - val_precision: 0.9434 - val_recall: 0.5842 - val_fmeasure: 0.7180\n",
      "Epoch 486/500\n",
      "7s - loss: 0.0135 - acc: 0.9991 - precision: 0.9990 - recall: 0.9821 - fmeasure: 0.9904 - val_loss: 0.0770 - val_acc: 0.9774 - val_precision: 0.9450 - val_recall: 0.5828 - val_fmeasure: 0.7175\n",
      "Epoch 487/500\n",
      "7s - loss: 0.0135 - acc: 0.9991 - precision: 0.9989 - recall: 0.9829 - fmeasure: 0.9907 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9438 - val_recall: 0.5844 - val_fmeasure: 0.7182\n",
      "Epoch 488/500\n",
      "7s - loss: 0.0135 - acc: 0.9990 - precision: 0.9992 - recall: 0.9805 - fmeasure: 0.9896 - val_loss: 0.0766 - val_acc: 0.9774 - val_precision: 0.9444 - val_recall: 0.5836 - val_fmeasure: 0.7179\n",
      "Epoch 489/500\n",
      "7s - loss: 0.0135 - acc: 0.9990 - precision: 0.9990 - recall: 0.9814 - fmeasure: 0.9900 - val_loss: 0.0766 - val_acc: 0.9776 - val_precision: 0.9432 - val_recall: 0.5887 - val_fmeasure: 0.7213\n",
      "Epoch 490/500\n",
      "7s - loss: 0.0136 - acc: 0.9990 - precision: 0.9990 - recall: 0.9808 - fmeasure: 0.9897 - val_loss: 0.0763 - val_acc: 0.9776 - val_precision: 0.9446 - val_recall: 0.5867 - val_fmeasure: 0.7202\n",
      "Epoch 491/500\n",
      "7s - loss: 0.0135 - acc: 0.9990 - precision: 0.9984 - recall: 0.9812 - fmeasure: 0.9896 - val_loss: 0.0762 - val_acc: 0.9776 - val_precision: 0.9441 - val_recall: 0.5872 - val_fmeasure: 0.7205\n",
      "Epoch 492/500\n",
      "7s - loss: 0.0135 - acc: 0.9990 - precision: 0.9989 - recall: 0.9814 - fmeasure: 0.9899 - val_loss: 0.0768 - val_acc: 0.9774 - val_precision: 0.9442 - val_recall: 0.5838 - val_fmeasure: 0.7179\n",
      "Epoch 493/500\n",
      "7s - loss: 0.0134 - acc: 0.9991 - precision: 0.9989 - recall: 0.9836 - fmeasure: 0.9910 - val_loss: 0.0765 - val_acc: 0.9775 - val_precision: 0.9444 - val_recall: 0.5856 - val_fmeasure: 0.7194\n",
      "Epoch 494/500\n",
      "7s - loss: 0.0134 - acc: 0.9991 - precision: 0.9990 - recall: 0.9824 - fmeasure: 0.9905 - val_loss: 0.0763 - val_acc: 0.9776 - val_precision: 0.9435 - val_recall: 0.5878 - val_fmeasure: 0.7208\n",
      "Epoch 495/500\n",
      "7s - loss: 0.0133 - acc: 0.9991 - precision: 0.9989 - recall: 0.9834 - fmeasure: 0.9909 - val_loss: 0.0765 - val_acc: 0.9775 - val_precision: 0.9436 - val_recall: 0.5862 - val_fmeasure: 0.7195\n",
      "Epoch 496/500\n",
      "7s - loss: 0.0134 - acc: 0.9990 - precision: 0.9984 - recall: 0.9807 - fmeasure: 0.9894 - val_loss: 0.0770 - val_acc: 0.9774 - val_precision: 0.9435 - val_recall: 0.5842 - val_fmeasure: 0.7179\n",
      "Epoch 497/500\n",
      "7s - loss: 0.0133 - acc: 0.9990 - precision: 0.9985 - recall: 0.9824 - fmeasure: 0.9902 - val_loss: 0.0767 - val_acc: 0.9774 - val_precision: 0.9438 - val_recall: 0.5846 - val_fmeasure: 0.7184\n",
      "Epoch 498/500\n",
      "7s - loss: 0.0133 - acc: 0.9990 - precision: 0.9988 - recall: 0.9817 - fmeasure: 0.9900 - val_loss: 0.0766 - val_acc: 0.9774 - val_precision: 0.9426 - val_recall: 0.5854 - val_fmeasure: 0.7187\n",
      "Epoch 499/500\n",
      "7s - loss: 0.0133 - acc: 0.9991 - precision: 0.9989 - recall: 0.9824 - fmeasure: 0.9904 - val_loss: 0.0766 - val_acc: 0.9775 - val_precision: 0.9444 - val_recall: 0.5855 - val_fmeasure: 0.7194\n",
      "Epoch 500/500\n",
      "7s - loss: 0.0133 - acc: 0.9990 - precision: 0.9985 - recall: 0.9819 - fmeasure: 0.9900 - val_loss: 0.0767 - val_acc: 0.9775 - val_precision: 0.9440 - val_recall: 0.5852 - val_fmeasure: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff60412d7d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 500\n",
    "\n",
    "model.fit(X_train_padded, y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          verbose=2, # 1 for process bar (might crash jupyter notebook)\n",
    "          validation_data=(X_test_padded, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Examples for other models\n",
    "\n",
    "* https://github.com/fchollet/keras/blob/1.2.1/examples/reuters_mlp.py\n",
    "* http://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "* http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras\n",
    "* https://github.com/fchollet/keras/blob/1.2.1/examples/imdb_cnn.py\n",
    "* https://github.com/fchollet/keras/blob/1.2.1/examples/imdb_lstm.py\n",
    "* https://github.com/fchollet/keras/blob/1.2.1/examples/imdb_bidirectional_lstm.py\n",
    "* https://github.com/fchollet/keras/blob/1.2.1/examples/imdb_cnn_lstm.py\n",
    "* https://github.com/explosion/spaCy/blob/v1.6.0/examples/deep_learning_keras.py#L102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Quick BOW Naive Bayes baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.94602851323828918, 0.98532791232101824, 0.9652783790804399, None)\n",
      "(0.82634358496427462, 0.70631970260223054, 0.76163206871868294, None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1,1))),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(alpha=0.001)))])\n",
    "classifier.fit(train_texts, y_train)\n",
    "train_pred = classifier.predict(train_texts)\n",
    "test_pred = classifier.predict(test_texts)\n",
    "print precision_recall_fscore_support(y_train, train_pred, average=\"micro\")\n",
    "print precision_recall_fscore_support(y_test, test_pred, average=\"micro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
